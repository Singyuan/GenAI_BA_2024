{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Learning Training Process\n",
        "\n",
        "In Deep Learning, before starting the training, you need to define the **model**, the **loss function** (criterion), and the **optimizer**. The training process typically follows four key steps:\n",
        "\n",
        "#### 1. Define the Model, Criterion, and Optimizer\n",
        "\n",
        "- The **model** represents the architecture of your neural network.\n",
        "- The **criterion** is the loss function that measures how far the model's predictions are from the true values. e.g. `nn.MSELoss()`, `nn.CrossEntropyLoss()`\n",
        "- The **optimizer** is used to update the model's parameters based on the gradients. e.g. `optim.SGD()`, `optim.Adam()`\n",
        "\n",
        "#### 2. Forward Pass: Pass the inputs through the model, compute the outputs, and calculate the loss.\n",
        "```\n",
        "outputs = model(inputs)\n",
        "loss = criterion(outputs, targets)\n",
        "```\n",
        "#### 3. Backward Pass: Compute gradients of the loss with respect to model parameters.\n",
        "```\n",
        "loss.backward()\n",
        "```\n",
        "#### 4. Update Parameters: Update the model parameters using the optimizer.\n",
        "```\n",
        "optimizer.step()\n",
        "```\n",
        "#### 5. Reset Gradients: Clear the computed gradients to avoid accumulation.\n",
        "```\n",
        "optimizer.zero_grad()\n",
        "```\n",
        "![training](https://hackmd.io/_uploads/BkWLuMNhye.png)"
      ],
      "metadata": {
        "id": "uJJ3UCoN3-_x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVotrSp5y-BH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abe1489a-980b-4192-f011-c60a60988754"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchsummary import summary\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "# Define a simple model and data\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(10, 2),\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm1d(2),\n",
        "    nn.Linear(2, 1)\n",
        ").to(device)\n",
        "criterion = nn.MSELoss()  # Mean Squared Error Loss\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)  # Stochastic Gradient Descent\n",
        "\n",
        "# Generate dummy data\n",
        "inputs = torch.randn(4, 10).to(device)  # 32 samples, 10 features\n",
        "targets = torch.randn(4, 1).to(device)  # 32 target values\n",
        "\n",
        "# Forward pass\n",
        "outputs = model(inputs)\n",
        "loss = criterion(outputs, targets)  # Compute the loss\n",
        "\n",
        "# Backward pass\n",
        "loss.backward()  # Compute gradients for all parameters\n",
        "\n",
        "# Update parameters\n",
        "optimizer.step()  # Apply the gradients to update model weights\n",
        "\n",
        "# Reset gradient\n",
        "optimizer.zero_grad()  # Clear gradients for the next iteration"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary(model, input_size=(10,))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twyHb6hhwV2d",
        "outputId": "4b0f9fe6-42f9-4301-8afc-bf7e2b4f47ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Linear-1                    [-1, 2]              22\n",
            "              ReLU-2                    [-1, 2]               0\n",
            "       BatchNorm1d-3                    [-1, 2]               4\n",
            "            Linear-4                    [-1, 1]               3\n",
            "================================================================\n",
            "Total params: 29\n",
            "Trainable params: 29\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.00\n",
            "Params size (MB): 0.00\n",
            "Estimated Total Size (MB): 0.00\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def show_grad(model) -> None:\n",
        "    for i, layer in enumerate(model):\n",
        "        if isinstance(layer, nn.Linear): # Check for Linear layer\n",
        "            print(f\"Layer {i} (Linear) weights gradient:\")\n",
        "            print(layer.weight.grad)\n",
        "            print(f\"Layer {i} (Linear) bias gradient:\")\n",
        "            print(layer.bias.grad)\n",
        "        elif isinstance(layer, nn.BatchNorm1d): # Check for BatchNorm1d layer\n",
        "            print(f\"Layer {i} (BatchNorm1d) running mean:\")\n",
        "            print(layer.running_mean.grad)\n",
        "            print(f\"Layer {i} (BatchNorm1d) running var:\")\n",
        "            print(layer.running_var.grad)\n",
        "    return None"
      ],
      "metadata": {
        "id": "zk95V521wYVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_param(model) -> None:\n",
        "    for i, layer in enumerate(model):\n",
        "        if isinstance(layer, nn.Linear): # Check for Linear layer\n",
        "            print(f\"Layer {i} (Linear) weights:\")\n",
        "            print(layer.weight)\n",
        "            print(f\"Layer {i} (Linear) bias:\")\n",
        "            print(layer.bias)\n",
        "        elif isinstance(layer, nn.BatchNorm1d): # Check for BatchNorm1d layer\n",
        "            print(f\"Layer {i} (BatchNorm1d) running mean:\")\n",
        "            print(layer.running_mean)\n",
        "            print(f\"Layer {i} (BatchNorm1d) running var:\")\n",
        "            print(layer.running_var)\n",
        "    return None"
      ],
      "metadata": {
        "id": "1i0H8VSSwYS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_size(model) -> None:\n",
        "    for i, layer in enumerate(model):\n",
        "        if isinstance(layer, nn.Linear): # Check for Linear layer\n",
        "            print(f\"Layer {i} (Linear) weights:\")\n",
        "            print(layer.weight.size())\n",
        "            print(f\"Layer {i} (Linear) bias:\")\n",
        "            print(layer.bias.size())\n",
        "        elif isinstance(layer, nn.BatchNorm1d): # Check for BatchNorm1d layer\n",
        "            print(f\"Layer {i} (BatchNorm1d) running mean:\")\n",
        "            print(layer.running_mean.size())\n",
        "            print(f\"Layer {i} (BatchNorm1d) running var:\")\n",
        "            print(layer.running_var.size())\n",
        "    return None"
      ],
      "metadata": {
        "id": "k6mM6T4TwYQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_size(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "geFpJXrQwYNb",
        "outputId": "95ec8412-852f-408f-9e2e-f151821bc897"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer 0 (Linear) weights:\n",
            "torch.Size([2, 10])\n",
            "Layer 0 (Linear) bias:\n",
            "torch.Size([2])\n",
            "Layer 2 (BatchNorm1d) running mean:\n",
            "torch.Size([2])\n",
            "Layer 2 (BatchNorm1d) running var:\n",
            "torch.Size([2])\n",
            "Layer 3 (Linear) weights:\n",
            "torch.Size([1, 2])\n",
            "Layer 3 (Linear) bias:\n",
            "torch.Size([1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's look at when gradients are calculated and when they are reset."
      ],
      "metadata": {
        "id": "_aXpuusV9syS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer.zero_grad()\n",
        "show_grad(model)\n",
        "outputs = model(inputs)\n",
        "loss = criterion(outputs, targets)\n",
        "\n",
        "print(\"=========== backward: compute gradient without update weight ===========\")\n",
        "loss.backward()\n",
        "show_grad(model)\n",
        "\n",
        "print(\"=========== update weights ===========\")\n",
        "optimizer.step()\n",
        "show_grad(model)\n",
        "\n",
        "print(\"=========== clear gradient ===========\")\n",
        "optimizer.zero_grad()\n",
        "show_grad(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oviqCT_8wYKk",
        "outputId": "0efdaf6e-118e-457e-ccfe-647fd4e2c7d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer 0 (Linear) weights gradient:\n",
            "None\n",
            "Layer 0 (Linear) bias gradient:\n",
            "None\n",
            "Layer 2 (BatchNorm1d) running mean:\n",
            "None\n",
            "Layer 2 (BatchNorm1d) running var:\n",
            "None\n",
            "Layer 3 (Linear) weights gradient:\n",
            "None\n",
            "Layer 3 (Linear) bias gradient:\n",
            "None\n",
            "=========== backward: compute gradient without update weight ===========\n",
            "Layer 0 (Linear) weights gradient:\n",
            "tensor([[-0.0887,  0.0704,  0.6894, -0.2819, -0.0098,  0.1812,  0.2640, -0.5957,\n",
            "          0.0465,  0.1058],\n",
            "        [ 1.3334,  0.7219,  0.7804, -0.6931, -0.9008,  0.8042, -0.8149,  1.6782,\n",
            "         -2.0290,  0.4211]])\n",
            "Layer 0 (Linear) bias gradient:\n",
            "tensor([-1.9388e-01,  1.1921e-07])\n",
            "Layer 2 (BatchNorm1d) running mean:\n",
            "None\n",
            "Layer 2 (BatchNorm1d) running var:\n",
            "None\n",
            "Layer 3 (Linear) weights gradient:\n",
            "tensor([[-0.2336,  2.0318]])\n",
            "Layer 3 (Linear) bias gradient:\n",
            "tensor([-0.5723])\n",
            "=========== update weights ===========\n",
            "Layer 0 (Linear) weights gradient:\n",
            "tensor([[-0.0887,  0.0704,  0.6894, -0.2819, -0.0098,  0.1812,  0.2640, -0.5957,\n",
            "          0.0465,  0.1058],\n",
            "        [ 1.3334,  0.7219,  0.7804, -0.6931, -0.9008,  0.8042, -0.8149,  1.6782,\n",
            "         -2.0290,  0.4211]])\n",
            "Layer 0 (Linear) bias gradient:\n",
            "tensor([-1.9388e-01,  1.1921e-07])\n",
            "Layer 2 (BatchNorm1d) running mean:\n",
            "None\n",
            "Layer 2 (BatchNorm1d) running var:\n",
            "None\n",
            "Layer 3 (Linear) weights gradient:\n",
            "tensor([[-0.2336,  2.0318]])\n",
            "Layer 3 (Linear) bias gradient:\n",
            "tensor([-0.5723])\n",
            "=========== clear gradient ===========\n",
            "Layer 0 (Linear) weights gradient:\n",
            "None\n",
            "Layer 0 (Linear) bias gradient:\n",
            "None\n",
            "Layer 2 (BatchNorm1d) running mean:\n",
            "None\n",
            "Layer 2 (BatchNorm1d) running var:\n",
            "None\n",
            "Layer 3 (Linear) weights gradient:\n",
            "None\n",
            "Layer 3 (Linear) bias gradient:\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's look at when parameters are updated."
      ],
      "metadata": {
        "id": "y-dD54EF93fN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer.zero_grad()\n",
        "show_param(model)\n",
        "outputs = model(inputs)\n",
        "loss = criterion(outputs, targets)\n",
        "\n",
        "print(\"=========== backward: compute gradient without update weight ===========\")\n",
        "loss.backward()\n",
        "show_param(model)\n",
        "\n",
        "print(\"=========== update weights ===========\")\n",
        "optimizer.step()\n",
        "show_param(model)\n",
        "\n",
        "print(\"=========== clear gradient ===========\")\n",
        "optimizer.zero_grad()\n",
        "show_param(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCrcxSVkwYHu",
        "outputId": "80900af9-b64a-4593-c0c0-c06c1e73a996"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer 0 (Linear) weights:\n",
            "Parameter containing:\n",
            "tensor([[ 0.1525,  0.0137,  0.2033, -0.2471, -0.1581, -0.0719, -0.2868,  0.2001,\n",
            "          0.2577,  0.1084],\n",
            "        [ 0.1827,  0.0307, -0.1462,  0.0191,  0.1154, -0.3269,  0.0037,  0.2141,\n",
            "          0.1603,  0.1684]], requires_grad=True)\n",
            "Layer 0 (Linear) bias:\n",
            "Parameter containing:\n",
            "tensor([ 0.1291, -0.0575], requires_grad=True)\n",
            "Layer 2 (BatchNorm1d) running mean:\n",
            "tensor([0.1426, 0.1207])\n",
            "Layer 2 (BatchNorm1d) running var:\n",
            "tensor([0.8626, 0.7573])\n",
            "Layer 3 (Linear) weights:\n",
            "Parameter containing:\n",
            "tensor([[-0.4544, -0.6435]], requires_grad=True)\n",
            "Layer 3 (Linear) bias:\n",
            "Parameter containing:\n",
            "tensor([-0.3612], requires_grad=True)\n",
            "=========== backward: compute gradient without update weight ===========\n",
            "Layer 0 (Linear) weights:\n",
            "Parameter containing:\n",
            "tensor([[ 0.1525,  0.0137,  0.2033, -0.2471, -0.1581, -0.0719, -0.2868,  0.2001,\n",
            "          0.2577,  0.1084],\n",
            "        [ 0.1827,  0.0307, -0.1462,  0.0191,  0.1154, -0.3269,  0.0037,  0.2141,\n",
            "          0.1603,  0.1684]], requires_grad=True)\n",
            "Layer 0 (Linear) bias:\n",
            "Parameter containing:\n",
            "tensor([ 0.1291, -0.0575], requires_grad=True)\n",
            "Layer 2 (BatchNorm1d) running mean:\n",
            "tensor([0.1881, 0.1671])\n",
            "Layer 2 (BatchNorm1d) running var:\n",
            "tensor([0.8501, 0.7011])\n",
            "Layer 3 (Linear) weights:\n",
            "Parameter containing:\n",
            "tensor([[-0.4544, -0.6435]], requires_grad=True)\n",
            "Layer 3 (Linear) bias:\n",
            "Parameter containing:\n",
            "tensor([-0.3612], requires_grad=True)\n",
            "=========== update weights ===========\n",
            "Layer 0 (Linear) weights:\n",
            "Parameter containing:\n",
            "tensor([[ 0.1534,  0.0129,  0.1963, -0.2442, -0.1580, -0.0738, -0.2894,  0.2060,\n",
            "          0.2571,  0.1072],\n",
            "        [ 0.1729,  0.0265, -0.1485,  0.0231,  0.1213, -0.3324,  0.0107,  0.2003,\n",
            "          0.1745,  0.1665]], requires_grad=True)\n",
            "Layer 0 (Linear) bias:\n",
            "Parameter containing:\n",
            "tensor([ 0.1310, -0.0575], requires_grad=True)\n",
            "Layer 2 (BatchNorm1d) running mean:\n",
            "tensor([0.1881, 0.1671])\n",
            "Layer 2 (BatchNorm1d) running var:\n",
            "tensor([0.8501, 0.7011])\n",
            "Layer 3 (Linear) weights:\n",
            "Parameter containing:\n",
            "tensor([[-0.4529, -0.6650]], requires_grad=True)\n",
            "Layer 3 (Linear) bias:\n",
            "Parameter containing:\n",
            "tensor([-0.3557], requires_grad=True)\n",
            "=========== clear gradient ===========\n",
            "Layer 0 (Linear) weights:\n",
            "Parameter containing:\n",
            "tensor([[ 0.1534,  0.0129,  0.1963, -0.2442, -0.1580, -0.0738, -0.2894,  0.2060,\n",
            "          0.2571,  0.1072],\n",
            "        [ 0.1729,  0.0265, -0.1485,  0.0231,  0.1213, -0.3324,  0.0107,  0.2003,\n",
            "          0.1745,  0.1665]], requires_grad=True)\n",
            "Layer 0 (Linear) bias:\n",
            "Parameter containing:\n",
            "tensor([ 0.1310, -0.0575], requires_grad=True)\n",
            "Layer 2 (BatchNorm1d) running mean:\n",
            "tensor([0.1881, 0.1671])\n",
            "Layer 2 (BatchNorm1d) running var:\n",
            "tensor([0.8501, 0.7011])\n",
            "Layer 3 (Linear) weights:\n",
            "Parameter containing:\n",
            "tensor([[-0.4529, -0.6650]], requires_grad=True)\n",
            "Layer 3 (Linear) bias:\n",
            "Parameter containing:\n",
            "tensor([-0.3557], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Advanced Skills"
      ],
      "metadata": {
        "id": "1RtqZ5FP4QZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.Sequential(\n",
        "    nn.Linear(10, 2),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(2, 1)\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "rKb4NoDi18qI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementing Deep Learning Functions Using NumPy\n",
        "Deep Learning is fundamentally a series of functions and operations applied to data. In this approach, we will implement basic deep learning functions using the NumPy library, leveraging your previous knowledge of matrix operations."
      ],
      "metadata": {
        "id": "FBquXn9W44iA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# Define the model\n",
        "\n",
        "# Create some input data\n",
        "inputs = torch.randn(4, 10).to(device)  # Batch of 4 samples, each with 10 features\n",
        "targets = torch.randn(4, 1).to(device)  # Target values\n",
        "\n",
        "# Function to perform matrix multiplication using\n",
        "def matmul_with_weights_and_bias_numpy(model, inputs):\n",
        "    inputs = inputs.cpu().numpy()\n",
        "    W1 = model[0].weight.detach().cpu().numpy()  # Shape: (2, 10)\n",
        "    b1 = model[0].bias.detach().cpu().numpy()    # Shape: (2,)\n",
        "    W2 = model[2].weight.detach().cpu().numpy()  # Shape: (1, 2)\n",
        "    b2 = model[2].bias.detach().cpu().numpy()    # Shape: (1,)\n",
        "\n",
        "    # First matrix multiplication: input @ W1.T (Shape: (4, 2))\n",
        "    output1 = np.dot(inputs, W1.T) + b1  # Shape: (4, 2)\n",
        "\n",
        "    # Apply ReLU after the first Linear layer (ReLU activation)\n",
        "    output1_relu = np.maximum(output1, 0)\n",
        "\n",
        "   # Second matrix multiplication: output1_relu @ W2.T (Shape: (4, 1))\n",
        "    output2 = np.dot(output1_relu, W2.T) + b2  # Shape: (4, 1)\n",
        "\n",
        "    return output2  # The final output, shape (4, 1)\n",
        "\n",
        "# Perform matrix multiplication with weights and bias\n",
        "matmul_output = matmul_with_weights_and_bias_numpy(model, inputs)\n",
        "\n",
        "# Forward pass through the model\n",
        "model_output = model(inputs)\n",
        "\n",
        "# Compare the result\n",
        "print(\"MatMul output:\", np.round(matmul_output, 4))\n",
        "print(\"Model output:\", model_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xfUDM3bwYEX",
        "outputId": "001ce27d-f1a2-4aa1-d59e-ff40422c0362"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MatMul output: [[1.2576]\n",
            " [0.6159]\n",
            " [0.4845]\n",
            " [0.497 ]]\n",
            "Model output: tensor([[1.2576],\n",
            "        [0.6159],\n",
            "        [0.4845],\n",
            "        [0.4970]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Understanding requires_grad in PyTorch\n",
        "The `requires_grad` attribute in PyTorch enables automatic differentiation by tracking operations on tensors, allowing gradients to be computed for optimization. You can try commenting or enabling `inputs.requires_grad = True` to observe the effect on gradient computation."
      ],
      "metadata": {
        "id": "iQacj8UJ50Ui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate dummy data\n",
        "inputs = torch.randn(4, 10).to(device)\n",
        "targets = torch.randn(4, 1).to(device)\n",
        "inputs.requires_grad = True # turn on or turn off\n",
        "print(inputs)\n",
        "\n",
        "# Forward pass\n",
        "outputs = model(inputs)\n",
        "loss = criterion(outputs, targets)  # Compute the loss\n",
        "optimizer = optim.SGD([{'params': model.parameters()}, {'params': inputs}], lr=1.)\n",
        "\n",
        "# Backward pass\n",
        "loss.backward()  # Compute gradients for all parameters\n",
        "\n",
        "# Update parameters\n",
        "optimizer.step()  # Apply the gradients to update model weights\n",
        "print(inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNNujs0awX80",
        "outputId": "6f491272-58ba-417a-ce79-cb44da51e51e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.4534, -1.0645,  0.2839, -0.9921, -1.0600,  0.5219,  0.5143,  1.2848,\n",
            "         -0.2122,  0.9502],\n",
            "        [-1.3286,  1.7345,  1.9765, -0.4719, -0.5561,  1.4850,  0.5428,  0.9903,\n",
            "         -1.0181, -0.4368],\n",
            "        [ 0.4171, -0.4225,  0.5338,  0.6584,  1.0168, -0.3858, -0.6769, -1.1842,\n",
            "          0.1727,  1.6155],\n",
            "        [-0.2080, -1.7594, -0.3703, -1.6607,  0.3330,  0.4466,  0.1733,  0.6349,\n",
            "          1.5357, -0.8100]], requires_grad=True)\n",
            "tensor([[ 1.2547, -1.0372,  0.5301, -0.9289, -0.9684,  0.5801,  0.6536,  1.1904,\n",
            "         -0.3777,  0.8683],\n",
            "        [-1.3286,  1.7345,  1.9765, -0.4719, -0.5561,  1.4850,  0.5428,  0.9903,\n",
            "         -1.0181, -0.4368],\n",
            "        [ 0.6101, -0.0342,  0.1301,  0.6279,  0.7152, -0.4082, -0.3423, -1.2325,\n",
            "         -0.0639,  1.3403],\n",
            "        [-0.5198, -1.7166,  0.0163, -1.5615,  0.4768,  0.5381,  0.3919,  0.4866,\n",
            "          1.2759, -0.9386]], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using torch.no_grad() for Inference in PyTorch\n",
        "`torch.no_grad()` disables gradient tracking, which reduces memory usage and computation during inference or validation. This is particularly useful when you don't need to update model parameters."
      ],
      "metadata": {
        "id": "_B4VTF567XaN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.randn(4, 10).to(device)\n",
        "targets = torch.randn(4, 1).to(device)\n",
        "optimizer.zero_grad()\n",
        "with torch.no_grad():\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, targets)\n",
        "\n",
        "show_grad(model)\n",
        "# Backward pass\n",
        "loss.backward()  # Compute gradients for all parameters"
      ],
      "metadata": {
        "id": "JVO9_-py3Ca-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "outputId": "7350cd7e-d163-445d-baef-8ca878153d7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer 0 (Linear) weights gradient:\n",
            "None\n",
            "Layer 0 (Linear) bias gradient:\n",
            "None\n",
            "Layer 2 (Linear) weights gradient:\n",
            "None\n",
            "Layer 2 (Linear) bias gradient:\n",
            "None\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-e257e69d87da>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mshow_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Compute gradients for all parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_L3oS3vo60BZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}