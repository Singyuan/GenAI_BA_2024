{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch\n",
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHibglGhyai5",
        "outputId": "5a050eb8-a8b4-4b67-dcb4-d06dbe8961d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m92.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.4.1-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.4.1-py3-none-any.whl (487 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m487.4/487.4 kB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-3.4.1 dill-0.3.8 multiprocess-0.70.16 xxhash-3.5.0\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from transformers import AutoModelForSequenceClassification"
      ],
      "metadata": {
        "id": "2s5YMm5nvQEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "config = {\n",
        "    'seed': 42,      # Your seed number, you can pick your lucky number. :)\n",
        "    'batch_size': 16,\n",
        "    'learning_rate': 1e-5,\n",
        "    'nhead': 4,\n",
        "    'd_model': 256,\n",
        "    'dim_feedforward': 256,\n",
        "    '_n_data': 64\n",
        "\n",
        "}\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "np.random.seed(config['seed'])\n",
        "torch.manual_seed(config['seed'])\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(config['seed'])"
      ],
      "metadata": {
        "id": "04OqB0uTxDel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset\n",
        "|text | label|\n",
        "|----|----|\n",
        "|I rented I AM CURIOUS-YELLOW from my video sto...|      0|\n",
        "|\"I Am Curious: Yellow\" is a risible and preten... |     0|\n",
        "\n",
        "\n",
        "- [Kaggle](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews)\n",
        "- [Paper](https://ai.stanford.edu/~amaas/papers/wvSent_acl2011.pdf)"
      ],
      "metadata": {
        "id": "1loh-fhlv5Oo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"imdb\")"
      ],
      "metadata": {
        "id": "NJUEWCLLd5Lo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345,
          "referenced_widgets": [
            "60aeaafe2bd24ae1876fdabb7204e03f",
            "2b26b7a231764d5793a3755b592e2c2a",
            "819dbcb1e4ef460dbbc9a92065695172",
            "98b83623aa3c41e284a13e740abf126d",
            "debb224da5184e9d81fca13847f9d5b9",
            "0f6cb0f75c6246ea8afd013a22d3c2e9",
            "2fc0a49d41974d72aacd2931b62c5f28",
            "d3190022a5e54a8db28af7a329349aeb",
            "586ccbc9d8da47a880ddf787529f64aa",
            "3d0000c0419a4e048d8bc0f1bb0016a2",
            "9f5574fcb62745f780b36a5afde5315e",
            "53325fd91cf4458eb1ab6ff142c8967a",
            "9d4ed43874eb4960a5d56ab0b980b781",
            "0336257d58a74b579a0ee72bc6aa7088",
            "186847c99bed4512b87d8b7f1b2b6b51",
            "eb9704174bc94ba9a6d3c0764fe557b7",
            "f4ac3d5d8ba04520847c2ed557beb977",
            "f2efcbd83fd04f4586f46138de3098e6",
            "ad32a5950d0245e3bb3709e4a292b3bf",
            "eef80908c81746ecb6ec37fca91bab2b",
            "ef557501e1174cd1a91a1f7ffa6f5aa7",
            "cde053209cd042c896742b534906802c",
            "74d6b10943a64b41b47de1bd2ed6e112",
            "edf7b642d2d94a5593111d64c80b7f1a",
            "95edd205772d429f896baaa9d17d0a50",
            "b82ff76c97ed4c89815d2dc87973b31c",
            "fbce824b89e84b2280cf68003097c682",
            "95b2275fa96447c8afdf3551bd5bcfb4",
            "39fb857ea8b548738580a044153ed1c0",
            "d7faf2648737491a993c011daa0dac7b",
            "3de78502ae674ae4907cd89fb05b849f",
            "3fe005ea0b1544ea8d8aa0038487473a",
            "ce9c1fe192d9470ea3188ee85350f82d",
            "3f28c4060dde4236b4f2fd3fb07f485d",
            "4c49564fe21f46759c739ed7dec9a6be",
            "7d87e0116b33474383b1168a0265389f",
            "24151502352f42b1932bf3ca76b64138",
            "a9f67c7134a3413fbcfec3ad3686e652",
            "cdfec8cfd7fa4f398c8c79895450f2b7",
            "df861db5ef424f4aa5c5a2e1d3acd90c",
            "626aff9f382142298adb59cfa4de4b31",
            "fa3b96ba64ad4262904a441f9aed75ce",
            "347492f3fcc04287a0bcaaab4fc9ccf3",
            "59702b7fb07e430aad3c9c4c29d0e1bc",
            "741a87045b744434956c1a8286fc00c7",
            "fd6964e10f6f4978998a9b3752bad479",
            "e53d99ab62774f13b181ec48ee6cadeb",
            "884a8464d21d42488beeef52f9b6acfd",
            "fd594649e78b4e658bba3840fe3cb94b",
            "b63c901d3a8745eb861d94aa5ee633e9",
            "310801760c3d4516b77932e3060d8ee1",
            "c53a4e3131844ab5ac9bd47d5ff2c697",
            "b588b3a288374163b1c431db48528379",
            "6de08035385141feaf790a393be7f6b7",
            "e93a3d547b904af697bbf7f8ad20bbfc",
            "eedac98453914b75a1ce20b924fb83b9",
            "771f8aeb4f9b489f9aff5245b7014af8",
            "f3d1666e98de49e6a4a2ffb1171832cf",
            "84a8e46353eb44bd8687d2399af6e1e3",
            "69b02ae735694f5395e9e688c92dda4b",
            "b74ec1d0a87046dbb3131b324eeda590",
            "e99573113ab3445c9441f6073a999ffe",
            "543d18fe10ac43b18454749861510920",
            "3b3e063c9e1749d6ab6b8bf2c5c2d70d",
            "4b75832841b5455ca4484c4cf2abd3a3",
            "13a77c883ce54e7da29fccfbebdc0ab9",
            "65f0e2369a3d4c00bfc5f3e3834912c1",
            "e4fe4ee8978244c0bdb5c1c521853b9c",
            "543342327568499f804218d858a5682f",
            "c862cb50c10d4ee7a44ce727a548b13b",
            "55d7d8e927904cc6a963c8afa157bdaf",
            "5cac5d3875d041a9b621cf31111f3cf9",
            "3728d81d17ff40d68db7a8765be9b828",
            "5ce87cb24ee9445499e1159876facc90",
            "ae0967cdd35c498c99604ae3a0edbaf6",
            "443d39b4416f4cb98c1d361e61621c28",
            "7baa4eee28fd4385a89a3bec9f399e3e"
          ]
        },
        "outputId": "80176354-a5a9-44d2-8c5b-f92c8c29f4b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/7.81k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "60aeaafe2bd24ae1876fdabb7204e03f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00000-of-00001.parquet:   0%|          | 0.00/21.0M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "53325fd91cf4458eb1ab6ff142c8967a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "test-00000-of-00001.parquet:   0%|          | 0.00/20.5M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "74d6b10943a64b41b47de1bd2ed6e112"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "unsupervised-00000-of-00001.parquet:   0%|          | 0.00/42.0M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3f28c4060dde4236b4f2fd3fb07f485d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "741a87045b744434956c1a8286fc00c7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eedac98453914b75a1ce20b924fb83b9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "65f0e2369a3d4c00bfc5f3e3834912c1"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the structure of the dataset\n",
        "print(dataset)\n",
        "\n",
        "# Convert the training and test data to pandas DataFrames\n",
        "train_df = pd.DataFrame(dataset['train'])\n",
        "test_df = pd.DataFrame(dataset['test'])\n",
        "\n",
        "# Display the structure of the training DataFrame\n",
        "print(\"Training DataFrame structure:\")\n",
        "print(train_df.head())\n",
        "\n",
        "# Display the structure of the test DataFrame\n",
        "print(\"Test DataFrame structure:\")\n",
        "print(test_df.head())\n",
        "\n",
        "# You can also check the size of the dataset\n",
        "print(f\"Training dataset size: {len(dataset['train'])}\")\n",
        "print(f\"Test dataset size: {len(dataset['test'])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iDWtYVxewyy",
        "outputId": "0b91ce45-d145-4f4f-ddd8-b01842d63fb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 25000\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 25000\n",
            "    })\n",
            "    unsupervised: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 50000\n",
            "    })\n",
            "})\n",
            "Training DataFrame structure:\n",
            "                                                text  label\n",
            "0  I rented I AM CURIOUS-YELLOW from my video sto...      0\n",
            "1  \"I Am Curious: Yellow\" is a risible and preten...      0\n",
            "2  If only to avoid making this type of film in t...      0\n",
            "3  This film was probably inspired by Godard's Ma...      0\n",
            "4  Oh, brother...after hearing about this ridicul...      0\n",
            "Test DataFrame structure:\n",
            "                                                text  label\n",
            "0  I love sci-fi and am willing to put up with a ...      0\n",
            "1  Worth the entertainment value of a rental, esp...      0\n",
            "2  its a totally average film with a few semi-alr...      0\n",
            "3  STAR RATING: ***** Saturday Night **** Friday ...      0\n",
            "4  First off let me say, If you haven't enjoyed a...      0\n",
            "Training dataset size: 25000\n",
            "Test dataset size: 25000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizer\n",
        "Tokenizer is a tool or algorithm used in natural language processing (NLP) to break down text into smaller units called tokens. These tokens can be words, subwords, or characters depending on the tokenization approach. The choice of tokenizer **depends on the specific NLP task** and the nature of the language being processed.\n",
        "\n",
        "Example:\n",
        "Text: “How are you”\n",
        "\n",
        "- Character Tokenization -> [ “H”, “o”, “w”, “ ”, “a”, “r”, “e”, “ ”, “y”, “o”,”u”]\n",
        "- Word Tokenization -> [“How”, “are”, “you”]\n",
        "- Sentence Tokenization -> [“How are you”]\n",
        "- Subword Tokenization -> [“How”, “ a”, “re”, “ y”, “ou”]\n",
        "\n",
        "Please refer to [BPE](https://platform.openai.com/tokenizer)."
      ],
      "metadata": {
        "id": "xM8-rmzmjRFA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load tokenizer => coresponding to training model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "\n",
        "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=config['seed']).select(range(config['_n_data']))\n",
        "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=config['seed']).select(range(config['_n_data']))\n",
        "\n",
        "print(\"Vocabulary size:\", len(tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "1b0757a863d74af399146cbcb96de7e2",
            "acd4b4cf3e184356bfcbb7bafe51941e",
            "774a3087973445a09a71ea9ea8d3fe6e",
            "ac1052f126034d5c91ab3004c00a33e8",
            "f3e32865b7d24cdc97d28316dcfc9b13",
            "f20f0876039d485f90547854cbfc7f66",
            "6fcff85411254d5590e96becfcf91ef5",
            "24cc388e1b3148e9973b814eb4f5cf98",
            "b8a3ef28e9e94cca9da594946892e5f7",
            "7867f7c616a2413899c659ee215efb89",
            "0f91796732894ec5ae5c719d4ab22289"
          ]
        },
        "id": "GeZkwyjcv47J",
        "outputId": "d6185792-f23e-43a1-e78c-9d96c6b36d0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1b0757a863d74af399146cbcb96de7e2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 30522\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def collate_fn(batch):\n",
        "#     # This function processes a batch of data, stacking tensors and performing padding if necessary.\n",
        "#     input_ids = torch.tensor([item[\"input_ids\"] for item in batch], dtype=torch.long)\n",
        "#     attention_mask = torch.tensor([item[\"attention_mask\"] for item in batch], dtype=torch.long)\n",
        "#     labels = torch.tensor([item[\"label\"] for item in batch], dtype=torch.long)\n",
        "\n",
        "#     return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch\n",
        "\n",
        "def my_collate(batch):\n",
        "    # This function processes a batch of data, stacking tensors and performing padding if necessary.\n",
        "    input_ids = [item[\"input_ids\"] for item in batch]\n",
        "    attention_mask = [item[\"attention_mask\"] for item in batch]\n",
        "    labels = [item[\"label\"] for item in batch]\n",
        "\n",
        "    input_ids_padded = pad_sequence([torch.tensor(ids, dtype=torch.long) for ids in input_ids], batch_first=True, padding_value=0)\n",
        "    attention_mask_padded = pad_sequence([torch.tensor(mask, dtype=torch.long) for mask in attention_mask], batch_first=True, padding_value=0)\n",
        "    labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    return {\"input_ids\": input_ids_padded, \"attention_mask\": attention_mask_padded, \"labels\": labels}\n",
        "\n",
        "\n",
        "train_dataloader = DataLoader(small_train_dataset, batch_size=config['batch_size'], collate_fn=my_collate)\n",
        "eval_dataloader = DataLoader(small_eval_dataset, batch_size=config['batch_size'], collate_fn=my_collate)\n",
        "\n",
        "# Print out the data size.\n",
        "first_batch = next(iter(train_dataloader))\n",
        "\n",
        "# print shape\n",
        "print(\"(batch_size, sequence_length) vector\")\n",
        "print(f\"input_ids shape: {first_batch['input_ids'].size()}\")\n",
        "print(f\"attention_mask shape: {first_batch['attention_mask'].size()}\")\n",
        "print(f\"labels shape: {first_batch['labels'].size()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmg0B_kYwxmv",
        "outputId": "26614073-6cb9-4a6c-f3b1-23228adfa1bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(batch_size, sequence_length) vector\n",
            "input_ids shape: torch.Size([16, 512])\n",
            "attention_mask shape: torch.Size([16, 512])\n",
            "labels shape: torch.Size([16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-c1r19ZS0fE9",
        "outputId": "26503782-2df0-4e66-ade1-1653ba41e867"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
              "        num_rows: 25000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
              "        num_rows: 25000\n",
              "    })\n",
              "    unsupervised: Dataset({\n",
              "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
              "        num_rows: 50000\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets.set_format(type=\"pandas\")\n",
        "df = tokenized_datasets['train'][:]\n",
        "display(df.head())\n",
        "tokenized_datasets.reset_format()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "685e5leg0lDK",
        "outputId": "ae907667-00e0-41c5-f9d6-93ddc22cda53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                                text  label  \\\n",
              "0  I rented I AM CURIOUS-YELLOW from my video sto...      0   \n",
              "1  \"I Am Curious: Yellow\" is a risible and preten...      0   \n",
              "2  If only to avoid making this type of film in t...      0   \n",
              "3  This film was probably inspired by Godard's Ma...      0   \n",
              "4  Oh, brother...after hearing about this ridicul...      0   \n",
              "\n",
              "                                           input_ids  \\\n",
              "0  [101, 1045, 12524, 1045, 2572, 8025, 1011, 375...   \n",
              "1  [101, 1000, 1045, 2572, 8025, 1024, 3756, 1000...   \n",
              "2  [101, 2065, 2069, 2000, 4468, 2437, 2023, 2828...   \n",
              "3  [101, 2023, 2143, 2001, 2763, 4427, 2011, 2643...   \n",
              "4  [101, 2821, 1010, 2567, 1012, 1012, 1012, 2044...   \n",
              "\n",
              "                                      token_type_ids  \\\n",
              "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "\n",
              "                                      attention_mask  \n",
              "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
              "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
              "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
              "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
              "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-004331ea-ed4b-4cfa-aee2-9a0c0d2f97d9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>input_ids</th>\n",
              "      <th>token_type_ids</th>\n",
              "      <th>attention_mask</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I rented I AM CURIOUS-YELLOW from my video sto...</td>\n",
              "      <td>0</td>\n",
              "      <td>[101, 1045, 12524, 1045, 2572, 8025, 1011, 375...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\"I Am Curious: Yellow\" is a risible and preten...</td>\n",
              "      <td>0</td>\n",
              "      <td>[101, 1000, 1045, 2572, 8025, 1024, 3756, 1000...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>If only to avoid making this type of film in t...</td>\n",
              "      <td>0</td>\n",
              "      <td>[101, 2065, 2069, 2000, 4468, 2437, 2023, 2828...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>This film was probably inspired by Godard's Ma...</td>\n",
              "      <td>0</td>\n",
              "      <td>[101, 2023, 2143, 2001, 2763, 4427, 2011, 2643...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Oh, brother...after hearing about this ridicul...</td>\n",
              "      <td>0</td>\n",
              "      <td>[101, 2821, 1010, 2567, 1012, 1012, 1012, 2044...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-004331ea-ed4b-4cfa-aee2-9a0c0d2f97d9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-004331ea-ed4b-4cfa-aee2-9a0c0d2f97d9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-004331ea-ed4b-4cfa-aee2-9a0c0d2f97d9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-59dab6fb-2538-403d-bfaa-798d4d034493\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-59dab6fb-2538-403d-bfaa-798d4d034493')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-59dab6fb-2538-403d-bfaa-798d4d034493 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"tokenized_datasets\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"\\\"I Am Curious: Yellow\\\" is a risible and pretentious steaming pile. It doesn't matter what one's political views are because this film can hardly be taken seriously on any level. As for the claim that frontal male nudity is an automatic NC-17, that isn't true. I've seen R-rated films with male nudity. Granted, they only offer some fleeting views, but where are the R-rated films with gaping vulvas and flapping labia? Nowhere, because they don't exist. The same goes for those crappy cable shows: schlongs swinging in the breeze but not a clitoris in sight. And those pretentious indie movies like The Brown Bunny, in which we're treated to the site of Vincent Gallo's throbbing johnson, but not a trace of pink visible on Chloe Sevigny. Before crying (or implying) \\\"double-standard\\\" in matters of nudity, the mentally obtuse should take into account one unavoidably obvious anatomical difference between men and women: there are no genitals on display when actresses appears nude, and the same cannot be said for a man. In fact, you generally won't see female genitals in an American film in anything short of porn or explicit erotica. This alleged double-standard is less a double standard than an admittedly depressing ability to come to terms culturally with the insides of women's bodies.\",\n          \"Oh, brother...after hearing about this ridiculous film for umpteen years all I can think of is that old Peggy Lee song..<br /><br />\\\"Is that all there is??\\\" ...I was just an early teen when this smoked fish hit the U.S. I was too young to get in the theater (although I did manage to sneak into \\\"Goodbye Columbus\\\"). Then a screening at a local film museum beckoned - Finally I could see this film, except now I was as old as my parents were when they schlepped to see it!!<br /><br />The ONLY reason this film was not condemned to the anonymous sands of time was because of the obscenity case sparked by its U.S. release. MILLIONS of people flocked to this stinker, thinking they were going to see a sex film...Instead, they got lots of closeups of gnarly, repulsive Swedes, on-street interviews in bland shopping malls, asinie political pretension...and feeble who-cares simulated sex scenes with saggy, pale actors.<br /><br />Cultural icon, holy grail, historic artifact..whatever this thing was, shred it, burn it, then stuff the ashes in a lead box!<br /><br />Elite esthetes still scrape to find value in its boring pseudo revolutionary political spewings..But if it weren't for the censorship scandal, it would have been ignored, then forgotten.<br /><br />Instead, the \\\"I Am Blank, Blank\\\" rhythymed title was repeated endlessly for years as a titilation for porno films (I am Curious, Lavender - for gay films, I Am Curious, Black - for blaxploitation films, etc..) and every ten years or so the thing rises from the dead, to be viewed by a new generation of suckers who want to see that \\\"naughty sex film\\\" that \\\"revolutionized the film industry\\\"...<br /><br />Yeesh, avoid like the plague..Or if you MUST see it - rent the video and fast forward to the \\\"dirty\\\" parts, just to get it over with.<br /><br />\",\n          \"If only to avoid making this type of film in the future. This film is interesting as an experiment but tells no cogent story.<br /><br />One might feel virtuous for sitting thru it because it touches on so many IMPORTANT issues but it does so without any discernable motive. The viewer comes away with no new perspectives (unless one comes up with one while one's mind wanders, as it will invariably do during this pointless film).<br /><br />One might better spend one's time staring out a window at a tree growing.<br /><br />\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"input_ids\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"token_type_ids\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"attention_mask\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Special Token | Token ID (BERT) | Description                                  |\n",
        "|---------------|-----------------|----------------------------------------------|\n",
        "| `[PAD]`       | 0               | Padding token, used to pad sequences to the same length. |\n",
        "| `[UNK]`       | 100             | Unknown token, used for words not in the vocabulary. |\n",
        "| `[CLS]`       | 101             | Classification token, used at the start of a sentence for classification tasks. |\n",
        "| `[SEP]`       | 102             | Separator token, used to separate two sequences or mark the end of a sequence. |\n",
        "| `[MASK]`      | 103             | Mask token, used in masked language modeling to represent a missing word. |\n"
      ],
      "metadata": {
        "id": "WcBOFlKBwuzA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(tokenized_datasets[\"train\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "vqGqW4xK1iRr",
        "outputId": "840eb525-c189-41a5-e5a1-6e6746472328"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.',\n",
              " 'label': 0,\n",
              " 'input_ids': [101,\n",
              "  1045,\n",
              "  12524,\n",
              "  1045,\n",
              "  2572,\n",
              "  8025,\n",
              "  1011,\n",
              "  3756,\n",
              "  2013,\n",
              "  2026,\n",
              "  2678,\n",
              "  3573,\n",
              "  2138,\n",
              "  1997,\n",
              "  2035,\n",
              "  1996,\n",
              "  6704,\n",
              "  2008,\n",
              "  5129,\n",
              "  2009,\n",
              "  2043,\n",
              "  2009,\n",
              "  2001,\n",
              "  2034,\n",
              "  2207,\n",
              "  1999,\n",
              "  3476,\n",
              "  1012,\n",
              "  1045,\n",
              "  2036,\n",
              "  2657,\n",
              "  2008,\n",
              "  2012,\n",
              "  2034,\n",
              "  2009,\n",
              "  2001,\n",
              "  8243,\n",
              "  2011,\n",
              "  1057,\n",
              "  1012,\n",
              "  1055,\n",
              "  1012,\n",
              "  8205,\n",
              "  2065,\n",
              "  2009,\n",
              "  2412,\n",
              "  2699,\n",
              "  2000,\n",
              "  4607,\n",
              "  2023,\n",
              "  2406,\n",
              "  1010,\n",
              "  3568,\n",
              "  2108,\n",
              "  1037,\n",
              "  5470,\n",
              "  1997,\n",
              "  3152,\n",
              "  2641,\n",
              "  1000,\n",
              "  6801,\n",
              "  1000,\n",
              "  1045,\n",
              "  2428,\n",
              "  2018,\n",
              "  2000,\n",
              "  2156,\n",
              "  2023,\n",
              "  2005,\n",
              "  2870,\n",
              "  1012,\n",
              "  1026,\n",
              "  7987,\n",
              "  1013,\n",
              "  1028,\n",
              "  1026,\n",
              "  7987,\n",
              "  1013,\n",
              "  1028,\n",
              "  1996,\n",
              "  5436,\n",
              "  2003,\n",
              "  8857,\n",
              "  2105,\n",
              "  1037,\n",
              "  2402,\n",
              "  4467,\n",
              "  3689,\n",
              "  3076,\n",
              "  2315,\n",
              "  14229,\n",
              "  2040,\n",
              "  4122,\n",
              "  2000,\n",
              "  4553,\n",
              "  2673,\n",
              "  2016,\n",
              "  2064,\n",
              "  2055,\n",
              "  2166,\n",
              "  1012,\n",
              "  1999,\n",
              "  3327,\n",
              "  2016,\n",
              "  4122,\n",
              "  2000,\n",
              "  3579,\n",
              "  2014,\n",
              "  3086,\n",
              "  2015,\n",
              "  2000,\n",
              "  2437,\n",
              "  2070,\n",
              "  4066,\n",
              "  1997,\n",
              "  4516,\n",
              "  2006,\n",
              "  2054,\n",
              "  1996,\n",
              "  2779,\n",
              "  25430,\n",
              "  14728,\n",
              "  2245,\n",
              "  2055,\n",
              "  3056,\n",
              "  2576,\n",
              "  3314,\n",
              "  2107,\n",
              "  2004,\n",
              "  1996,\n",
              "  5148,\n",
              "  2162,\n",
              "  1998,\n",
              "  2679,\n",
              "  3314,\n",
              "  1999,\n",
              "  1996,\n",
              "  2142,\n",
              "  2163,\n",
              "  1012,\n",
              "  1999,\n",
              "  2090,\n",
              "  4851,\n",
              "  8801,\n",
              "  1998,\n",
              "  6623,\n",
              "  7939,\n",
              "  4697,\n",
              "  3619,\n",
              "  1997,\n",
              "  8947,\n",
              "  2055,\n",
              "  2037,\n",
              "  10740,\n",
              "  2006,\n",
              "  4331,\n",
              "  1010,\n",
              "  2016,\n",
              "  2038,\n",
              "  3348,\n",
              "  2007,\n",
              "  2014,\n",
              "  3689,\n",
              "  3836,\n",
              "  1010,\n",
              "  19846,\n",
              "  1010,\n",
              "  1998,\n",
              "  2496,\n",
              "  2273,\n",
              "  1012,\n",
              "  1026,\n",
              "  7987,\n",
              "  1013,\n",
              "  1028,\n",
              "  1026,\n",
              "  7987,\n",
              "  1013,\n",
              "  1028,\n",
              "  2054,\n",
              "  8563,\n",
              "  2033,\n",
              "  2055,\n",
              "  1045,\n",
              "  2572,\n",
              "  8025,\n",
              "  1011,\n",
              "  3756,\n",
              "  2003,\n",
              "  2008,\n",
              "  2871,\n",
              "  2086,\n",
              "  3283,\n",
              "  1010,\n",
              "  2023,\n",
              "  2001,\n",
              "  2641,\n",
              "  26932,\n",
              "  1012,\n",
              "  2428,\n",
              "  1010,\n",
              "  1996,\n",
              "  3348,\n",
              "  1998,\n",
              "  16371,\n",
              "  25469,\n",
              "  5019,\n",
              "  2024,\n",
              "  2261,\n",
              "  1998,\n",
              "  2521,\n",
              "  2090,\n",
              "  1010,\n",
              "  2130,\n",
              "  2059,\n",
              "  2009,\n",
              "  1005,\n",
              "  1055,\n",
              "  2025,\n",
              "  2915,\n",
              "  2066,\n",
              "  2070,\n",
              "  10036,\n",
              "  2135,\n",
              "  2081,\n",
              "  22555,\n",
              "  2080,\n",
              "  1012,\n",
              "  2096,\n",
              "  2026,\n",
              "  2406,\n",
              "  3549,\n",
              "  2568,\n",
              "  2424,\n",
              "  2009,\n",
              "  16880,\n",
              "  1010,\n",
              "  1999,\n",
              "  4507,\n",
              "  3348,\n",
              "  1998,\n",
              "  16371,\n",
              "  25469,\n",
              "  2024,\n",
              "  1037,\n",
              "  2350,\n",
              "  18785,\n",
              "  1999,\n",
              "  4467,\n",
              "  5988,\n",
              "  1012,\n",
              "  2130,\n",
              "  13749,\n",
              "  7849,\n",
              "  24544,\n",
              "  1010,\n",
              "  15835,\n",
              "  2037,\n",
              "  3437,\n",
              "  2000,\n",
              "  2204,\n",
              "  2214,\n",
              "  2879,\n",
              "  2198,\n",
              "  4811,\n",
              "  1010,\n",
              "  2018,\n",
              "  3348,\n",
              "  5019,\n",
              "  1999,\n",
              "  2010,\n",
              "  3152,\n",
              "  1012,\n",
              "  1026,\n",
              "  7987,\n",
              "  1013,\n",
              "  1028,\n",
              "  1026,\n",
              "  7987,\n",
              "  1013,\n",
              "  1028,\n",
              "  1045,\n",
              "  2079,\n",
              "  4012,\n",
              "  3549,\n",
              "  2094,\n",
              "  1996,\n",
              "  16587,\n",
              "  2005,\n",
              "  1996,\n",
              "  2755,\n",
              "  2008,\n",
              "  2151,\n",
              "  3348,\n",
              "  3491,\n",
              "  1999,\n",
              "  1996,\n",
              "  2143,\n",
              "  2003,\n",
              "  3491,\n",
              "  2005,\n",
              "  6018,\n",
              "  5682,\n",
              "  2738,\n",
              "  2084,\n",
              "  2074,\n",
              "  2000,\n",
              "  5213,\n",
              "  2111,\n",
              "  1998,\n",
              "  2191,\n",
              "  2769,\n",
              "  2000,\n",
              "  2022,\n",
              "  3491,\n",
              "  1999,\n",
              "  26932,\n",
              "  12370,\n",
              "  1999,\n",
              "  2637,\n",
              "  1012,\n",
              "  1045,\n",
              "  2572,\n",
              "  8025,\n",
              "  1011,\n",
              "  3756,\n",
              "  2003,\n",
              "  1037,\n",
              "  2204,\n",
              "  2143,\n",
              "  2005,\n",
              "  3087,\n",
              "  5782,\n",
              "  2000,\n",
              "  2817,\n",
              "  1996,\n",
              "  6240,\n",
              "  1998,\n",
              "  14629,\n",
              "  1006,\n",
              "  2053,\n",
              "  26136,\n",
              "  3832,\n",
              "  1007,\n",
              "  1997,\n",
              "  4467,\n",
              "  5988,\n",
              "  1012,\n",
              "  2021,\n",
              "  2428,\n",
              "  1010,\n",
              "  2023,\n",
              "  2143,\n",
              "  2987,\n",
              "  1005,\n",
              "  1056,\n",
              "  2031,\n",
              "  2172,\n",
              "  1997,\n",
              "  1037,\n",
              "  5436,\n",
              "  1012,\n",
              "  102,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0],\n",
              " 'token_type_ids': [0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0],\n",
              " 'attention_mask': [1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0]}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embedding\n",
        "Example of tokenizing a sentence using the pre-trained BERT model 'bert-base-uncased.'\n",
        "Please refer to [CSDN](https://blog.csdn.net/zhaohongfei_358/article/details/122809709) for more details.\n"
      ],
      "metadata": {
        "id": "zeT3NGG51jAe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
        "embedding_layer = model.get_input_embeddings()\n",
        "\n",
        "# Get the token ID for '[PAD]'\n",
        "pad_token_id = tokenizer.pad_token_id  # this is typically 0 for BERT\n",
        "pad_embedding = embedding_layer.weight[pad_token_id]\n",
        "\n",
        "# Print the embedding vector for '[PAD]' token\n",
        "print(pad_embedding.size())\n",
        "print(f\"Embedding for [PAD] token (token ID {pad_token_id}):\\n{pad_embedding}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "7bdc32f3a8bc4b068494c34ce6ff9ad4",
            "3bc128ff27b343d790bebf2b60510cff",
            "9ced2354ae0947af995ec94163db8c13",
            "95c3589aa79c445d9bd7f8d3dc10121f",
            "a6ba8858c6874977ac5dac1a9c0588c8",
            "9dc76d4745e5468abb93f502af8a521e",
            "c9aaede2cc0d4615b49baf4c4ed65f27",
            "d190601a8a8d420f8693e5995559349c",
            "2e6cb58224eb48c59cfae278f1b8e517",
            "fabb1a07c5c04a56a715a256dcdac378",
            "4de9eb5e31c147d4bc836199efc4f641"
          ]
        },
        "id": "_fOjlAWg1iZ8",
        "outputId": "2fb8b17d-9a70-44d8-8953-c15e67d49a11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7bdc32f3a8bc4b068494c34ce6ff9ad4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([768])\n",
            "Embedding for [PAD] token (token ID 0):\n",
            "tensor([-1.0183e-02, -6.1549e-02, -2.6497e-02, -4.2061e-02,  1.1672e-03,\n",
            "        -2.8272e-02, -4.4500e-02, -2.2465e-02, -4.6553e-03, -8.2129e-02,\n",
            "        -5.0238e-03, -4.6508e-02, -4.9514e-02,  2.1517e-02, -1.6588e-02,\n",
            "        -3.7279e-02, -7.2888e-02, -4.6671e-02,  1.9787e-03, -5.5847e-02,\n",
            "        -2.8919e-02, -2.2304e-02, -4.4846e-03, -1.5506e-02, -1.0986e-01,\n",
            "        -2.6746e-02,  8.3565e-03, -5.3755e-02,  8.1516e-03, -2.5817e-02,\n",
            "        -2.8301e-02, -2.6342e-03, -1.7270e-02, -1.7444e-02, -5.0403e-02,\n",
            "        -5.4036e-02, -3.3925e-02, -1.9397e-02, -6.2235e-02, -1.9178e-03,\n",
            "        -3.0086e-02, -3.1459e-02, -5.0693e-02, -1.8174e-02,  6.8573e-03,\n",
            "        -8.9839e-03, -1.1808e-02, -3.2866e-02, -3.8003e-03, -2.7472e-02,\n",
            "        -3.3144e-02, -1.6076e-02, -5.8682e-02,  1.0107e-01, -2.9100e-02,\n",
            "        -2.4062e-02, -1.5432e-02,  5.2106e-03, -2.3103e-03,  4.4728e-03,\n",
            "        -1.1664e-02, -1.4309e-02,  1.0915e-01, -4.0001e-02, -2.9073e-02,\n",
            "        -1.1655e-02, -2.0877e-02, -3.0113e-02, -6.7081e-03, -3.3477e-02,\n",
            "        -3.2437e-02, -2.2434e-02, -4.2970e-02, -6.0056e-02, -7.4236e-02,\n",
            "        -1.3895e-02, -6.1835e-02,  1.0269e-01, -3.9120e-03, -4.6541e-02,\n",
            "        -3.0964e-02, -3.0767e-02,  2.5584e-03, -1.2184e-02, -1.6253e-02,\n",
            "        -8.0201e-03, -1.9821e-02, -5.0253e-02, -1.4750e-02, -1.5226e-02,\n",
            "        -5.7067e-02,  1.5486e-02, -3.9450e-02, -1.6222e-02, -1.8338e-02,\n",
            "        -5.4316e-02,  1.0489e-01, -2.2203e-02, -2.7777e-02, -4.5925e-02,\n",
            "        -1.5566e-02, -1.9080e-02,  1.0311e-02, -3.6669e-02, -4.7115e-02,\n",
            "         1.0181e-01, -4.6930e-02, -1.2128e-02,  6.5257e-03, -1.2827e-02,\n",
            "        -3.8169e-02, -6.9440e-03, -1.1407e-02, -3.5235e-02, -1.9060e-02,\n",
            "        -1.0307e-02, -1.5898e-02, -4.1200e-02, -4.6643e-02, -1.6302e-02,\n",
            "        -6.0176e-03,  1.3129e-01, -2.1036e-02, -2.2117e-02, -1.4402e-01,\n",
            "        -2.8599e-02, -1.3696e-02, -1.6189e-01, -2.8067e-02, -4.7525e-02,\n",
            "         1.5784e-03, -2.8012e-02,  1.2878e-01,  1.0251e-02, -4.4899e-02,\n",
            "        -1.5743e-02, -1.8824e-02, -2.2676e-02, -1.0056e-02,  1.0403e-01,\n",
            "        -2.7887e-02, -3.1321e-02, -4.4232e-02, -2.5753e-03, -2.5393e-02,\n",
            "        -1.6916e-02, -1.3631e-02, -7.0404e-02, -5.0128e-02, -4.1968e-02,\n",
            "        -1.9662e-02, -2.9982e-02, -9.7740e-02, -2.7181e-02, -4.2037e-02,\n",
            "        -4.5557e-02, -1.0865e-02, -2.0161e-02, -2.8992e-02,  1.0530e-01,\n",
            "        -2.8693e-02, -3.2648e-02, -2.6289e-02, -4.8735e-02, -3.3725e-02,\n",
            "        -3.6585e-02, -2.3407e-03, -1.2749e-02,  1.2790e-01, -3.8057e-02,\n",
            "        -4.1076e-02,  8.9528e-03,  3.8439e-03, -7.4162e-03, -4.3910e-03,\n",
            "         1.1115e-01, -2.6632e-02, -3.8905e-02, -2.6805e-03, -2.5154e-03,\n",
            "        -1.4362e-02, -3.3516e-02, -7.3486e-02, -2.2472e-02, -2.1929e-02,\n",
            "        -4.9053e-02, -1.3212e-02, -2.8033e-02, -2.1171e-02, -2.5870e-02,\n",
            "        -2.0398e-02, -3.1615e-02, -7.5224e-02, -3.3260e-02, -4.6676e-02,\n",
            "        -4.7785e-02, -4.6732e-02, -3.5637e-02, -5.9978e-02, -5.0052e-02,\n",
            "        -6.1737e-02, -7.5817e-03,  9.6870e-02, -6.1890e-02, -2.1407e-02,\n",
            "        -1.2563e-02, -3.6797e-02, -3.3287e-02, -3.4421e-02, -1.3894e-02,\n",
            "        -8.8133e-03, -7.1193e-03, -2.9318e-02, -2.4419e-02, -5.7806e-03,\n",
            "         1.0094e-01,  3.6177e-02, -9.4556e-03,  1.0957e-01, -1.9263e-02,\n",
            "        -1.8295e-02, -1.3713e-02, -3.5285e-02, -2.1754e-02, -6.0722e-02,\n",
            "         7.4449e-02, -3.6236e-02, -3.5528e-02, -2.9701e-02, -4.3306e-02,\n",
            "        -2.8259e-02, -3.1988e-02, -7.5381e-02, -3.7412e-02, -2.5376e-02,\n",
            "        -1.5048e-02, -1.2630e-02,  3.0254e-03, -1.8867e-02, -3.7209e-02,\n",
            "        -4.2477e-02, -2.8853e-02, -4.1193e-02, -5.3924e-02, -4.0215e-02,\n",
            "         1.0739e-01, -8.1261e-03, -6.7805e-02, -1.0442e-02, -5.9944e-03,\n",
            "        -5.0754e-02, -3.4951e-02, -1.9209e-02,  2.4028e-02, -2.3215e-02,\n",
            "         9.1592e-03, -1.5822e-02, -3.6030e-02, -1.2390e-03, -1.5653e-02,\n",
            "        -2.5223e-02, -4.3194e-02,  3.0492e-04, -2.2742e-02, -6.7213e-02,\n",
            "        -1.3190e-02, -9.8063e-03, -3.9939e-02, -3.4624e-02, -3.2686e-02,\n",
            "        -6.0179e-02, -1.7002e-02,  6.2076e-03, -3.9388e-02, -3.4796e-02,\n",
            "        -2.0594e-02, -1.0612e-02, -4.9301e-02, -1.1104e-02, -5.2989e-02,\n",
            "        -2.8889e-02, -3.0242e-02, -2.2861e-02,  8.7022e-02, -5.0955e-03,\n",
            "        -4.8337e-02, -3.7352e-02, -3.6109e-02, -3.7013e-02, -2.8155e-02,\n",
            "        -7.7945e-02,  1.0369e-01, -3.3813e-03, -3.2303e-02, -5.1843e-02,\n",
            "        -1.2638e-02,  7.5395e-02, -4.1105e-02, -8.1194e-02, -3.7080e-02,\n",
            "        -2.9148e-02, -4.2149e-02, -2.0645e-02, -1.5752e-02, -6.2624e-02,\n",
            "        -9.1115e-03, -9.3606e-03, -3.9015e-02,  7.3948e-02, -6.6416e-02,\n",
            "        -2.1531e-02,  1.0344e-01, -1.9789e-02, -5.1098e-02, -4.5898e-02,\n",
            "        -5.9087e-02, -5.4325e-02, -2.0398e-02, -9.4863e-03,  1.0731e-01,\n",
            "        -3.6582e-02, -1.0492e-02, -2.4951e-02, -4.3366e-02, -5.0797e-02,\n",
            "        -7.4541e-03, -1.5912e-02, -3.7816e-02, -8.8382e-03,  9.4313e-03,\n",
            "        -2.6761e-02,  9.2137e-02, -6.0993e-02, -2.4813e-02, -1.7906e-02,\n",
            "        -1.6638e-02,  3.4132e-03, -4.7958e-02, -1.1274e-02, -1.4764e-02,\n",
            "        -2.9976e-02, -3.1712e-02, -3.1065e-02, -2.7403e-02,  3.8099e-03,\n",
            "         1.1391e-01, -3.0056e-02, -7.2062e-02, -4.2530e-02, -6.7686e-03,\n",
            "        -8.2922e-02, -2.2100e-02, -1.5664e-02,  8.6857e-02, -4.2564e-02,\n",
            "        -3.6770e-02, -3.2660e-02, -3.1075e-02, -1.7221e-02, -1.8290e-02,\n",
            "        -9.2484e-03, -4.2167e-02, -8.0546e-03, -3.2582e-02, -1.3629e-02,\n",
            "        -3.9236e-02, -3.0365e-02, -3.1991e-02, -4.8070e-02, -4.5202e-02,\n",
            "        -5.9905e-03, -6.9557e-02,  5.1678e-03, -3.6334e-02, -6.4922e-02,\n",
            "        -8.3255e-02, -2.8914e-02, -3.3091e-02, -2.6676e-02, -9.9082e-03,\n",
            "        -1.2852e-02, -4.4699e-02, -3.9713e-02, -1.3178e-02, -2.7917e-02,\n",
            "        -4.2105e-02, -1.1465e-02, -1.8148e-02, -2.5464e-02, -1.8177e-02,\n",
            "        -6.4978e-02, -3.5865e-02, -4.0396e-02, -2.4917e-02, -3.5268e-02,\n",
            "        -2.9050e-02, -2.9837e-02, -2.9454e-02, -5.8591e-02, -3.8190e-02,\n",
            "        -9.9292e-03, -1.4688e-01, -5.0589e-02, -4.6752e-02, -2.6712e-02,\n",
            "        -1.7390e-02, -3.5716e-02, -1.3478e-02, -1.9240e-02, -5.2646e-02,\n",
            "        -4.4020e-02, -4.3754e-02, -3.4104e-02, -6.1761e-02, -2.2163e-02,\n",
            "        -5.9165e-03, -4.4477e-02, -6.0515e-02, -1.6442e-02, -1.3964e-01,\n",
            "        -1.3091e-02,  2.9891e-03, -2.8450e-02, -4.0221e-02, -6.4843e-02,\n",
            "        -2.3305e-02, -2.1295e-02, -5.9680e-02, -2.7242e-02, -8.1213e-02,\n",
            "        -2.3816e-02, -3.1888e-02, -2.2602e-02,  7.1650e-04, -1.4583e-03,\n",
            "        -3.2853e-02, -1.5436e-02, -1.8040e-02, -4.0737e-03, -1.2500e-02,\n",
            "        -2.2526e-02,  6.1890e-03, -3.6514e-02,  1.1811e-01, -4.8195e-02,\n",
            "        -2.3736e-02, -2.8025e-02, -5.5344e-03, -5.0422e-02, -5.0358e-02,\n",
            "        -2.3664e-02, -2.4511e-02, -2.4658e-02, -1.6584e-02, -1.1389e-03,\n",
            "        -3.4519e-02, -4.2772e-02, -3.7253e-02, -2.1710e-02, -3.2764e-02,\n",
            "        -3.3882e-02,  8.0421e-02, -4.3115e-02, -3.1928e-03, -2.6939e-02,\n",
            "        -3.9356e-02, -3.5763e-02, -1.8438e-02,  4.0275e-03, -4.3807e-02,\n",
            "        -2.5287e-02, -7.5479e-02, -5.1585e-02, -5.5813e-03, -2.8903e-02,\n",
            "        -1.0321e-04, -1.7794e-02, -3.1067e-02, -2.9924e-02, -2.5176e-03,\n",
            "        -2.8445e-02, -9.2819e-03,  1.0094e-01,  1.6806e-03, -1.2409e-02,\n",
            "        -2.0125e-03,  2.8797e-04, -2.6442e-02, -1.9636e-02, -3.3840e-02,\n",
            "        -2.9655e-02, -3.7720e-02, -2.6476e-02, -3.5638e-02, -1.9039e-02,\n",
            "        -1.4205e-02,  3.4499e-02, -2.1957e-03, -5.5352e-02, -6.9782e-02,\n",
            "         5.3646e-03, -4.0639e-02, -3.7305e-02, -4.5545e-02, -8.0159e-02,\n",
            "        -5.8593e-02, -4.1835e-02, -7.8558e-03, -1.6702e-02, -4.3946e-02,\n",
            "        -1.9233e-02, -6.2756e-02, -3.7755e-02, -4.1372e-02, -2.3529e-02,\n",
            "        -3.5586e-02, -1.5953e-02,  1.0166e-01, -1.4708e-02, -3.0660e-02,\n",
            "        -4.6192e-02, -4.9449e-03, -1.2994e-03, -8.7631e-03, -4.4982e-03,\n",
            "        -2.7307e-02, -2.8870e-02, -3.4730e-02, -3.5924e-02,  1.3086e-01,\n",
            "        -5.1444e-02, -6.7933e-02, -2.5930e-02, -1.8110e-02, -1.4634e-02,\n",
            "        -1.8420e-02, -2.7314e-02, -3.9060e-02, -2.7538e-02, -1.3160e-02,\n",
            "        -3.6749e-02,  9.3712e-02, -1.2460e-02, -3.8781e-02, -3.6931e-03,\n",
            "         1.1223e-01,  1.5086e-03, -2.8322e-02, -5.1344e-02, -2.8439e-02,\n",
            "        -3.5813e-02, -1.5584e-02, -2.9601e-02, -1.4666e-02, -1.9192e-02,\n",
            "        -3.4605e-02, -9.5308e-03, -8.8686e-03, -3.9085e-02, -2.5009e-02,\n",
            "        -2.9764e-02, -1.6883e-02, -1.3655e-03, -3.1188e-02, -1.9257e-02,\n",
            "        -2.5113e-02, -5.1900e-02, -4.0914e-02, -7.4950e-02, -1.0425e-02,\n",
            "        -2.3747e-02, -5.3528e-02, -1.9618e-02, -1.5790e-02,  3.6146e-04,\n",
            "        -7.4964e-03, -3.5127e-02,  1.1713e-01, -1.6412e-02, -4.4679e-02,\n",
            "        -1.3890e-02, -6.1886e-02, -3.5083e-02,  1.5211e-03, -8.0316e-03,\n",
            "         3.2548e-03, -3.5813e-02, -2.9009e-02, -4.5562e-02, -8.6372e-02,\n",
            "        -3.0847e-02, -6.5691e-02, -3.0470e-02, -4.8908e-02, -1.2185e-02,\n",
            "         6.3106e-03, -3.1242e-02, -1.4366e-02, -5.4990e-02, -2.6072e-02,\n",
            "        -2.3195e-02, -1.9134e-02, -6.5817e-02,  1.3134e-02, -8.6765e-03,\n",
            "        -2.8602e-02, -6.6840e-03, -6.0905e-02, -4.5329e-02, -1.7936e-02,\n",
            "        -4.6084e-02, -6.0692e-02, -1.4765e-02, -1.0674e-02,  1.0575e-01,\n",
            "        -4.6400e-02, -1.3616e-02, -1.9578e-02, -3.8178e-02, -1.6190e-03,\n",
            "        -2.2568e-02, -3.8275e-02, -2.2664e-02, -3.9251e-02, -2.4218e-02,\n",
            "        -3.9837e-02, -3.9683e-02, -3.8449e-02, -8.9372e-03, -3.0485e-02,\n",
            "        -5.8538e-02, -9.6671e-03, -1.1454e-02, -3.5158e-02, -7.5105e-03,\n",
            "        -2.0650e-02, -4.6145e-03, -4.1925e-02, -1.5284e-02, -4.3931e-02,\n",
            "        -6.5469e-03, -5.1608e-02, -4.4916e-02,  3.0051e-03, -8.5406e-03,\n",
            "        -1.0299e-02, -2.7708e-02, -2.9062e-02, -1.2142e-02,  5.7758e-03,\n",
            "        -6.1638e-02, -2.3828e-02, -2.0867e-02, -5.1357e-02,  1.3327e-01,\n",
            "        -4.2220e-03, -1.4068e-02, -2.8133e-02, -1.0857e-02, -4.2404e-02,\n",
            "        -4.2291e-02, -5.4132e-02, -2.7479e-02, -4.1749e-02, -4.4311e-02,\n",
            "        -3.3302e-02, -3.9388e-03, -5.2169e-02, -2.4235e-02, -1.6820e-02,\n",
            "         1.2338e-01, -5.1071e-02, -4.6809e-02, -1.5621e-02, -2.8104e-02,\n",
            "        -2.3201e-02, -1.9918e-02, -3.0640e-02, -3.3774e-02, -9.1381e-03,\n",
            "        -1.9826e-03, -3.7010e-02, -3.4161e-02, -2.5863e-02, -4.4537e-02,\n",
            "        -2.1705e-02, -1.4065e-02, -2.7418e-02, -4.1339e-02, -9.9585e-03,\n",
            "        -3.6958e-02,  5.8991e-03, -3.1152e-02, -1.4280e-02, -3.5814e-02,\n",
            "        -1.4860e-02,  3.9596e-03, -1.1289e-02, -4.8315e-02,  5.8197e-02,\n",
            "        -8.1996e-03,  1.4357e-03, -3.0713e-02, -6.0927e-02, -5.6728e-02,\n",
            "        -3.5902e-02, -4.0188e-03, -2.9491e-03, -3.7171e-02, -4.8878e-02,\n",
            "        -6.3957e-03, -3.7394e-02, -2.1048e-02, -3.4480e-02, -1.0230e-02,\n",
            "        -2.0924e-02, -2.5572e-02,  2.5012e-03, -5.4340e-02, -2.3479e-03,\n",
            "        -1.6723e-02, -2.4130e-02, -5.0779e-02, -2.7017e-02, -3.9475e-02,\n",
            "        -4.3564e-02, -3.7715e-03, -3.5978e-02, -6.0178e-02, -2.2083e-02,\n",
            "        -3.9697e-02, -9.6672e-03, -1.9678e-02, -4.4540e-02, -6.2129e-02,\n",
            "        -1.3934e-03, -1.5755e-04, -7.7517e-03, -7.1707e-02,  8.3854e-04,\n",
            "        -4.0077e-02, -3.6744e-02, -1.5602e-02, -6.2337e-02, -1.9976e-02,\n",
            "        -2.9310e-02, -3.3579e-03, -3.0580e-02, -6.0318e-03, -1.6074e-02,\n",
            "        -2.1336e-02, -5.3142e-02, -6.7147e-03, -4.2129e-02, -4.7328e-02,\n",
            "         7.8782e-03, -6.8284e-02, -3.2570e-02, -4.3651e-02, -7.7066e-03,\n",
            "        -9.2494e-03, -3.1207e-02, -3.6753e-02, -3.8042e-02, -1.4242e-02,\n",
            "        -1.9854e-02, -3.7210e-02, -9.7515e-03], grad_fn=<SelectBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Loop"
      ],
      "metadata": {
        "id": "fRacb7zJ9v67"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "def train(model, train_dataloader, optimizer, criterion):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    for batch in tqdm(train_dataloader, desc=\"Training\", leave=True):\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        # get the model output\n",
        "        if isinstance(output, dict):\n",
        "            # this is for transformer\n",
        "            logits = output.get(\"logits\", None)\n",
        "        else:\n",
        "            # this is for user defined\n",
        "            logits = output\n",
        "\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(logits, 1)\n",
        "        correct = (predicted == labels).sum().item()\n",
        "        epoch_acc += correct / len(labels)\n",
        "\n",
        "    return epoch_loss / len(train_dataloader), epoch_acc / len(train_dataloader)\n",
        "\n",
        "def evaluate(model, eval_dataloader, criterion):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(eval_dataloader, desc=\"Evaluating\", leave=True):\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "\n",
        "            output = model(input_ids, attention_mask=attention_mask)\n",
        "            # get the model output\n",
        "            if isinstance(output, dict):\n",
        "                # this is for transformer\n",
        "                logits = output.get(\"logits\", None)\n",
        "            else:\n",
        "                # this is for user defined\n",
        "                logits = output\n",
        "            loss = criterion(logits, labels)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(logits, 1)\n",
        "            correct = (predicted == labels).sum().item()\n",
        "            epoch_acc += correct / len(labels)\n",
        "\n",
        "    return epoch_loss / len(eval_dataloader), epoch_acc / len(eval_dataloader)"
      ],
      "metadata": {
        "id": "lsECtCdw9vos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "PPmOgdaK2BZm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model_A: Encoder-Decoder Model\n",
        "Please refer to [知乎](https://zhuanlan.zhihu.com/p/338817680) or [github](https://github.com/hyunwoongko/transformer) for more details. In this [知乎](https://zhuanlan.zhihu.com/p/338817680), there is a detailed step-by-step diagram specifically explaining the autoregressive process of the decoder.\n",
        "- The decoder's `src` is the output from the encoder (the source sequence), while the `tgt` is the target sequence that the decoder generates step by step.\n",
        "- It keeps generating tokens autoregressively until it encounters the `[EOS]` token, using previously generated tokens as input for the next step.\n",
        "- Masking is applied to ensure the model only attends to past tokens, preventing it from looking ahead at future tokens during this process."
      ],
      "metadata": {
        "id": "8MChPO-s-ETe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class EncoderDecoderClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    Encoder-Decoder seq2label model: No auto regression\n",
        "    vocab_size: It represents the total number of unique tokens (words, subwords, or characters) which depends on the model.\n",
        "    d_model: the dimensionality of the token embeddings (i.e., each token is represented as a d_model-dimensional vector.\n",
        "    dim_feedforward: It defines the size of the hidden layer(s) in the feedforward neural network that follows the attention mechanism within each Transformer layer.\n",
        "    nhead: It specifies the number of attention heads in the multi-head self-attention mechanism, allowing the model to focus on different parts of the input sequence simultaneously.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 vocab_size,\n",
        "                 d_model=256,\n",
        "                 nhead=4,\n",
        "                 dim_feedforward=256,\n",
        "                 dropout=0.1,\n",
        "                 num_labels=2) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        # embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "        # Transformer encoder\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, dim_feedforward=dim_feedforward, nhead=nhead, batch_first=True)\n",
        "        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=2)\n",
        "\n",
        "        # Transformer decoder\n",
        "        self.decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, dim_feedforward=dim_feedforward, nhead=nhead, batch_first=True)\n",
        "        self.decoder = nn.TransformerDecoder(self.decoder_layer, num_layers=2)\n",
        "\n",
        "        # final classifer\n",
        "        self.pred_layer = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_model, num_labels),\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, decoder_input_ids=None):\n",
        "        # embedding\n",
        "        # input_ids: (batch_size, seq_length)\n",
        "        embedded = self.embedding(input_ids) # (batch_size, seq_length, d_model)\n",
        "        embedded = embedded.permute(1, 0, 2)  # (seq_length, batch_size, d_model)\n",
        "\n",
        "        # 1. Transformer encoder\n",
        "        encoded = self.encoder(embedded) # (seq_length, batch_size, d_model)\n",
        "\n",
        "        # 2. embedding\n",
        "        if decoder_input_ids is None:\n",
        "            # artificial prepare target sequence\n",
        "            decoder_input_ids = input_ids\n",
        "\n",
        "        decoder_embedded = self.embedding(decoder_input_ids) # (batch_size, seq_length, d_model)\n",
        "        decoder_embedded = decoder_embedded.permute(1, 0, 2)  # (seq_length, batch_size, d_model)\n",
        "\n",
        "        # 3. Transformer decoder\n",
        "        # src: encoded\n",
        "        # tgt: decoder_embedded\n",
        "        decoded = self.decoder(decoder_embedded, encoded) # (seq_length, batch_size, d_model)\n",
        "\n",
        "        # 4. artificial pool\n",
        "        pooled = decoded.mean(dim=0)\n",
        "\n",
        "        # 5. predict\n",
        "        output = self.pred_layer(pooled)\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "8TeziKt06d3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model_C: Decoder Model"
      ],
      "metadata": {
        "id": "6aOVeXt9POf7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Single Head Attention mechanism for the Transformer\n",
        "class SingleHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model: int, attention_dim: int):\n",
        "        super().__init__()\n",
        "        # d_model = embedded_dim\n",
        "        # Linear layers to generate key, query, and value vectors for attention\n",
        "        self.key_gen = nn.Linear(d_model, attention_dim, bias=False)\n",
        "        self.query_gen = nn.Linear(d_model, attention_dim, bias=False)\n",
        "        self.value_gen = nn.Linear(d_model, attention_dim, bias=False)\n",
        "\n",
        "    def forward(self, embedded: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        embedded: (batch_size, context_length, d_model)\n",
        "        attention_mask: (batch_size, context_length)\n",
        "        \"\"\"\n",
        "        # Generate key, query, and value vectors for self-attention\n",
        "        k = self.key_gen(embedded)\n",
        "        q = self.query_gen(embedded)\n",
        "        v = self.value_gen(embedded)\n",
        "\n",
        "        # Calculate attention scores (Q * K.T) and scale by sqrt(attention_dim)\n",
        "        # (batch_size, context_length, context_length)\n",
        "        scores = torch.matmul(q, k.transpose(-1, -2)) / (k.shape[-1] ** 0.5)\n",
        "\n",
        "        # Mask out future tokens (for causal attention in autoregressive models)\n",
        "        lower_tri = torch.tril(torch.ones(context_length, context_length)).to(device)\n",
        "        scores = scores.masked_fill(lower_tri == 0, float('-inf'))\n",
        "        # Mask the attention scores based on attention_mask (for padding)\n",
        "        # scores = scores.masked_fill(attention_mask == 0, float('-inf')) # (batch_size, context_length, context_length)\n",
        "\n",
        "        # Calculate the attention weights by applying softmax\n",
        "        attention_weights = torch.softmax(scores, dim=-1)\n",
        "\n",
        "        # Apply attention weights to the value vectors (Weighted sum of V)\n",
        "        # (batch_size, context_length, attention_dim)\n",
        "        return torch.matmul(attention_weights, v)\n",
        "\n",
        "\n",
        "# Define Multi-Head Self Attention mechanism for Transformer\n",
        "class MultiHeadedSelfAttention(nn.Module):\n",
        "    def __init__(self, d_model: int, nhead: int):\n",
        "        super().__init__()\n",
        "        # Create multiple single head attention layers\n",
        "        self.att_heads = nn.ModuleList([SingleHeadAttention(d_model, d_model // nhead) for _ in range(nhead)])\n",
        "\n",
        "    def forward(self, embedded: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        embedded: (batch_size, context_length, d_model)\n",
        "        attention_mask: (batch_size, context_length)\n",
        "        \"\"\"\n",
        "        # Expand the attention mask to match the shape of attention scores\n",
        "        # attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)  # Shape (batch_size, 1, 1, context_length)\n",
        "        attention_mask = (attention_mask == 0).float()  # Mark padding positions as 1 for masking\n",
        "\n",
        "        # Apply each attention head and concatenate the results\n",
        "        head_outputs = [head(embedded, attention_mask) for head in self.att_heads]\n",
        "        return torch.cat(head_outputs, dim=-1)\n",
        "\n",
        "\n",
        "# Define the Feed-Forward Neural Network (FFN) in Transformer\n",
        "class VanillaNeuralNetwork(nn.Module):\n",
        "    def __init__(self, d_model: int, dim_feedforward=256, dropout=0.1):\n",
        "        super().__init__()\n",
        "        # Projection layers in the feed-forward network\n",
        "        self.up_projection = nn.Linear(d_model, dim_feedforward)  # Upscale the dimension\n",
        "        self.relu = nn.ReLU()\n",
        "        self.down_projection = nn.Linear(dim_feedforward, d_model)  # Downscale the dimension\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # Feed input through the network with ReLU activation and dropout\n",
        "        return self.dropout(self.down_projection(self.relu(self.up_projection(x))))\n",
        "\n",
        "\n",
        "# Define a single Transformer Block\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model: int, nhead: int, dim_feedforward=256, dropout=0.1):\n",
        "        super().__init__()\n",
        "        # Instantiate multi-head self-attention and feed-forward network\n",
        "        self.attention = MultiHeadedSelfAttention(d_model, nhead)\n",
        "        self.linear_network = VanillaNeuralNetwork(d_model, dim_feedforward, dropout)\n",
        "        self.first_norm = nn.LayerNorm(d_model)  # Layer normalization before attention\n",
        "        self.second_norm = nn.LayerNorm(d_model)  # Layer normalization before feed-forward network\n",
        "\n",
        "    def forward(self, embedded: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
        "        # Add the output from attention to the input (residual connection)\n",
        "        embedded = embedded + self.attention(self.first_norm(embedded), attention_mask)\n",
        "        # Add the output from the feed-forward network to the input (residual connection)\n",
        "        embedded = embedded + self.linear_network(self.second_norm(embedded))\n",
        "        return embedded\n",
        "\n",
        "\n",
        "# Define the main GUPT model (GPT model without pre-trained)\n",
        "class GUPT(nn.Module):\n",
        "    \"\"\"\n",
        "    Generative un-pre-trained transformer (decoder)\n",
        "    vocab_size: It represents the total number of unique tokens (words, subwords, or characters) which depends on the model.\n",
        "    d_model: the dimensionality of the token embeddings (i.e., each token is represented as a d_model-dimensional vector.\n",
        "    nhead: It specifies the number of attention heads in the multi-head self-attention mechanism, allowing the model to focus on different parts of the input sequence simultaneously.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 vocab_size: int,\n",
        "                 context_length: int,\n",
        "                 d_model=256,\n",
        "                 nhead=4,\n",
        "                 dim_feedforward=256,\n",
        "                 dropout=0.1,\n",
        "                 num_blocks=6,\n",
        "                 num_labels=2) -> None:\n",
        "        super().__init__()\n",
        "        # Word embedding layer\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, d_model)\n",
        "        # Position embedding layer\n",
        "        self.position_embeddings = nn.Embedding(context_length, d_model)\n",
        "\n",
        "        # Set up the Transformer blocks\n",
        "        self.transformer_blocks = nn.ModuleList([TransformerBlock(d_model, nhead, dim_feedforward, dropout) for _ in range(num_blocks)])\n",
        "\n",
        "        # Layer normalization and classifier for final output\n",
        "        self.final_norm = nn.LayerNorm(d_model)\n",
        "        self.classifier = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # Prediction layer (final classification layer)\n",
        "        self.pred_layer = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model//2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_model//2, num_labels),\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
        "\n",
        "        # Calculate word and position embeddings\n",
        "        embedded = self.word_embeddings(input_ids)\n",
        "        context_length = input_ids.shape[1]\n",
        "        positions = torch.arange(context_length, device=input_ids.device)\n",
        "        embedded = embedded + self.position_embeddings(positions)\n",
        "\n",
        "        # Pass through all the Transformer blocks\n",
        "        for block in self.transformer_blocks:\n",
        "            embedded = block(embedded, attention_mask)\n",
        "\n",
        "        # Apply final LayerNorm and word projection\n",
        "        raw_output = self.classifier(self.final_norm(embedded))\n",
        "        # Apply mean pooling across the sequence to aggregate the output\n",
        "        pooled = raw_output.mean(dim=1)\n",
        "        pooled = self.pred_layer(pooled)\n",
        "\n",
        "        return pooled\n"
      ],
      "metadata": {
        "id": "Pm0iTzaAOFYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Experiment 1"
      ],
      "metadata": {
        "id": "LXNZVwZSBj4q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = EncoderDecoderClassifier(vocab_size=len(tokenizer), d_model=config['d_model'], nhead=config['nhead'], dim_feedforward=config['dim_feedforward'], num_labels=2).to(device)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=config['learning_rate'])"
      ],
      "metadata": {
        "id": "qsKw8Cbb2Hze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"======= before training =======\")\n",
        "eval_loss, eval_acc = evaluate(model, eval_dataloader, criterion)\n",
        "print(f\"Eval Loss: {eval_loss:.4f}, Eval Accuracy: {eval_acc * 100:.2f}%\")\n",
        "print(\"======= after training =======\")\n",
        "for epoch in range(3):\n",
        "    train_loss, train_acc = train(model, train_dataloader, optimizer, criterion)\n",
        "    eval_loss, eval_acc = evaluate(model, eval_dataloader, criterion)\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}\")\n",
        "    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc * 100:.2f}%\")\n",
        "    print(f\"Eval Loss: {eval_loss:.4f}, Eval Accuracy: {eval_acc * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIAm9lg694xl",
        "outputId": "e792433f-ff99-439a-b01f-ec15de3d40b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======= before training =======\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 4/4 [00:00<00:00,  4.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval Loss: 0.7048, Eval Accuracy: 46.88%\n",
            "======= after training =======\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 4/4 [00:00<00:00,  5.15it/s]\n",
            "Evaluating: 100%|██████████| 4/4 [00:00<00:00, 27.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "Train Loss: 0.7048, Train Accuracy: 46.88%\n",
            "Eval Loss: 0.7007, Eval Accuracy: 46.88%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 4/4 [00:00<00:00, 13.40it/s]\n",
            "Evaluating: 100%|██████████| 4/4 [00:00<00:00, 26.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2\n",
            "Train Loss: 0.6964, Train Accuracy: 46.88%\n",
            "Eval Loss: 0.6959, Eval Accuracy: 46.88%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 4/4 [00:00<00:00, 13.09it/s]\n",
            "Evaluating: 100%|██████████| 4/4 [00:00<00:00, 27.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3\n",
            "Train Loss: 0.6915, Train Accuracy: 42.19%\n",
            "Eval Loss: 0.6931, Eval Accuracy: 56.25%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Experiment 2"
      ],
      "metadata": {
        "id": "ehVoIqkDBwAq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained model\n",
        "model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2).to(device)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=config['learning_rate'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ni3DOCupw-T",
        "outputId": "1c7aa12e-24f2-4c43-b0d8-cd3f30b8ac4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"======= before training =======\")\n",
        "eval_loss, eval_acc = evaluate(model, eval_dataloader, criterion)\n",
        "print(f\"Eval Loss: {eval_loss:.4f}, Eval Accuracy: {eval_acc * 100:.2f}%\")\n",
        "print(\"======= after training =======\")\n",
        "for epoch in range(3):\n",
        "    train_loss, train_acc = train(model, train_dataloader, optimizer, criterion)\n",
        "    eval_loss, eval_acc = evaluate(model, eval_dataloader, criterion)\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}\")\n",
        "    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc * 100:.2f}%\")\n",
        "    print(f\"Eval Loss: {eval_loss:.4f}, Eval Accuracy: {eval_acc * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qswa8dqxCvSE",
        "outputId": "cd1bf8ff-a163-42dd-81dd-d531cab2d429"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======= before training =======\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 4/4 [00:01<00:00,  2.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval Loss: 0.7423, Eval Accuracy: 53.12%\n",
            "======= after training =======\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 4/4 [00:05<00:00,  1.36s/it]\n",
            "Evaluating: 100%|██████████| 4/4 [00:01<00:00,  2.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "Train Loss: 0.7334, Train Accuracy: 53.12%\n",
            "Eval Loss: 0.6959, Eval Accuracy: 54.69%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 4/4 [00:05<00:00,  1.37s/it]\n",
            "Evaluating: 100%|██████████| 4/4 [00:01<00:00,  2.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2\n",
            "Train Loss: 0.6454, Train Accuracy: 65.62%\n",
            "Eval Loss: 0.6826, Eval Accuracy: 59.38%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 4/4 [00:05<00:00,  1.40s/it]\n",
            "Evaluating: 100%|██████████| 4/4 [00:01<00:00,  2.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3\n",
            "Train Loss: 0.6339, Train Accuracy: 67.19%\n",
            "Eval Loss: 0.6702, Eval Accuracy: 64.06%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Experiement 3"
      ],
      "metadata": {
        "id": "09aZ6RnTDsH3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_length = tokenizer.model_max_length\n",
        "num_blocks = 6  # Transformer layer\n",
        "\n",
        "model = GUPT(vocab_size=len(tokenizer), context_length=context_length, d_model=config['d_model'], nhead=config['nhead'], dim_feedforward=config['dim_feedforward'], num_blocks=num_blocks, num_labels=2).to(device)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=config['learning_rate'])"
      ],
      "metadata": {
        "id": "_4vBTfaSDrrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "print(\"======= before training =======\")\n",
        "eval_loss, eval_acc = evaluate(model, eval_dataloader, criterion)\n",
        "print(f\"Eval Loss: {eval_loss:.4f}, Eval Accuracy: {eval_acc * 100:.2f}%\")\n",
        "print(\"======= after training =======\")\n",
        "for epoch in range(3):\n",
        "    train_loss, train_acc = train(model, train_dataloader, optimizer, criterion)\n",
        "    eval_loss, eval_acc = evaluate(model, eval_dataloader, criterion)\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}\")\n",
        "    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc * 100:.2f}%\")\n",
        "    print(f\"Eval Loss: {eval_loss:.4f}, Eval Accuracy: {eval_acc * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2EtUo_PrcUX",
        "outputId": "882bd454-8506-48e1-9451-664946af87e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======= before training =======\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 4/4 [00:00<00:00, 10.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval Loss: 0.6981, Eval Accuracy: 46.88%\n",
            "======= after training =======\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 4/4 [00:00<00:00,  6.24it/s]\n",
            "Evaluating: 100%|██████████| 4/4 [00:00<00:00, 12.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "Train Loss: 0.7028, Train Accuracy: 46.88%\n",
            "Eval Loss: 0.6985, Eval Accuracy: 46.88%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 4/4 [00:00<00:00,  6.48it/s]\n",
            "Evaluating: 100%|██████████| 4/4 [00:00<00:00, 12.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2\n",
            "Train Loss: 0.6989, Train Accuracy: 46.88%\n",
            "Eval Loss: 0.6982, Eval Accuracy: 46.88%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 4/4 [00:00<00:00,  6.78it/s]\n",
            "Evaluating: 100%|██████████| 4/4 [00:00<00:00, 12.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3\n",
            "Train Loss: 0.6958, Train Accuracy: 46.88%\n",
            "Eval Loss: 0.6978, Eval Accuracy: 46.88%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m32cTa1isncr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reference\n",
        "1. https://ithelp.ithome.com.tw/articles/10298638\n",
        "2. https://ithelp.ithome.com.tw/articles/10301854\n",
        "2. https://github.com/kapadias/medium-articles/blob/master/natural-language-processing/transformers-series/sentiment_analysis_bert.ipynb\n",
        "3. https://medium.com/ching-i/transformer-attention-is-all-you-need-c7967f38af14"
      ],
      "metadata": {
        "id": "GQFd_F6CWp5E"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AQ3X3pQZWrkj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}