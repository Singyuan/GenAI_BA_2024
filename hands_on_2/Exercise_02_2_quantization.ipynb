{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Quantization\n",
        "\n",
        "Please refer to [website](https://mlabonne.github.io/blog/posts/Introduction_to_Weight_Quantization.html), [HF](https://huggingface.co/blog/hf-bitsandbytes-integration) or [bnb](https://huggingface.co/docs/bitsandbytes/index) for more details."
      ],
      "metadata": {
        "id": "YDpKmpK2M_r4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers bitsandbytes accelerate"
      ],
      "metadata": {
        "id": "wnlrfcqMbKXG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6979641-1114-480e-d39c-475af313a59f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.3.0)\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-1.5.2-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n",
            "Downloading transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m101.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading accelerate-1.5.2-py3-none-any.whl (345 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.1/345.1 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m98.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m102.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, transformers, bitsandbytes, accelerate\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.48.3\n",
            "    Uninstalling transformers-4.48.3:\n",
            "      Successfully uninstalled transformers-4.48.3\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 1.3.0\n",
            "    Uninstalling accelerate-1.3.0:\n",
            "      Successfully uninstalled accelerate-1.3.0\n",
            "Successfully installed accelerate-1.5.2 bitsandbytes-0.45.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 transformers-4.49.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "9rf6imW8PRcE",
        "outputId": "bd24d4ca-db7a-42ae-8de0-1ff4454a4075",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331,
          "referenced_widgets": [
            "1dcf2b90a7374578af9adb2d88b7c588",
            "94603d527c474619bbc98daee0fc27cb",
            "3b1d0d3b51e742fa9d42a43c91f1c60b",
            "05db9f0f03c04a1d9706967e5bd9bdfe",
            "997e637fee5c45388d4decab98071f6e",
            "564b7ab17d194fdba3461093f32f2be7",
            "bad061e0258d4c0e8e92ac09950c5b20",
            "e9007a4fce644a6d86e356fa2b3ed2f4",
            "9309502b661847278bbabb31ce33745d",
            "5f75eb9bb73a469a91a5b76f993177cf",
            "40798cc24c55408e8e3e9605833bf2e6",
            "c9229d48fa7a4ce29f02d23e85a199a7",
            "9effe9067bb54ea68aa0ccbe5e04edf6",
            "e0d70558293e48c6b179605e4d76e189",
            "795901e5ff3f4e6a82c55bee312a8872",
            "e02cf66088cf431c97209ce58b479bec",
            "2a719fa743244b98960d80fc0d21e729"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1dcf2b90a7374578af9adb2d88b7c588"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import torch"
      ],
      "metadata": {
        "id": "WFJvxoXJA7gs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Comparison between Different Quantization using LLaAM-2\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5n5jgXSHtIJb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = 'meta-llama/Llama-2-7b-chat-hf'"
      ],
      "metadata": {
        "id": "wr19yI4Y8mYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Original model"
      ],
      "metadata": {
        "id": "wVmy_7V18ng0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the Model\n",
        "# base_model = AutoModelForCausalLM.from_pretrained(model_id).to(\"cuda\")"
      ],
      "metadata": {
        "id": "rASmvZILPtNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8-bit quantization\n",
        "In 8-bit quantization, model weights and activations are represented using 8 bits (values in the range of -128 to 127 for signed integers or 0 to 255 for unsigned integers). This means that the continuous floating-point numbers (typically 32-bit) are approximated using a much smaller range. To perform this approximation, two key concepts are used: scale factor and zero-point. Please refer to [arXiv](https://arxiv.org/abs/2208.07339) for more details."
      ],
      "metadata": {
        "id": "9T5h6NUpuQqy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# INT8 Config\n",
        "bnb_config_8bit = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,\n",
        ")\n",
        "\n",
        "model_8bit = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config_8bit, low_cpu_mem_usage=True)\n",
        "\n",
        "# Loading Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "print(f\"INT8 Model size: {model_8bit.get_memory_footprint()/1024./1024./1024.:,} GB\\n\")"
      ],
      "metadata": {
        "id": "1msnmxazPZla",
        "outputId": "f65cd9bd-7421-4b96-8735-b872c53dbc1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508,
          "referenced_widgets": [
            "801fa6a5394f48aa86a60ca83bf11a6d",
            "b3eac4cec99a4689998c361eaa245725",
            "151a4c2b4de544b39d1745e9f7630b16",
            "bfa2b1b3b90346c4a70ce45204928810",
            "adf855a0ee68442899571ef1e3e5aabc",
            "d6e751f25f6b481db206b89f11b2959b",
            "e3b520bfa4a6430280fcfcc6bc08d332",
            "822ef3e32e604bd0804648c5675e94c7",
            "41dd375e9c6048f6a6ac08cb9135b172",
            "0c56f557ffd34fd9956769f584304620",
            "1875ce8032344ebda642bb66b39d16ad",
            "536bb70efe1c4869a2af1d53d02feb60",
            "9f846ae62ee34101af5aca2e52dda709",
            "b5af4dfb4bbe4ef69018f74d0f14b463",
            "236bda0998f042bb800f257076c73e0b",
            "86f2ea0e59e64e54b46a4f4dc4022869",
            "f875b409519b4a7da12c891f27305428",
            "cf6e0e63d29d478dba410e8e46113796",
            "f4ce376895bc49da94a36b5077fd5837",
            "253736d05fcd428691eb26dc28eae216",
            "36d6c52f4a194dd4a22515b42d676837",
            "9f5c93fe9bb340f993649d531ce6fe02",
            "9c77c10ebd494652ade09436eb93b319",
            "f1a199c3f438403f9467655aa4b7649e",
            "62f489f43bf443ba803561e7d807d10a",
            "e0bb7a26c1bc4348bf48d47f38a88bde",
            "2e17596654a245d581f7bfb88d740b34",
            "3fa5d614312b47b2b7c9ce249f18b738",
            "bab3ee1d990b49539b305c92bd17dd7a",
            "06a1d5fc1810446b907c656c980ef834",
            "9672aaab7dc44c84af266af569e2b0cb",
            "37a22e8c53094005ae14c79aa2bdce80",
            "ff6042ebff364369bea90a828cf24651",
            "990062d42a3a426a9d5f6c96cf364ef3",
            "5974d75f77de47f79ba1f2359bef958f",
            "ca871046691f4360bd2a8e4c415a75f0",
            "0e51fc12a4bf4c159f4639712bd7b0d8",
            "617a6ac17e604185964ec727637d9a08",
            "0a509c6261604c3f9cd428363fa40692",
            "be65996718924825a537a99ee2e6c3f1",
            "bdb00bd170fc42e6adc3bf72ce2cd712",
            "3f84f7592a3e40d0b5e4bd9264432bca",
            "05c0ffd15ae64705938526882ce421d9",
            "75ea4cb792d64aaba9349b69df882e1e",
            "e35e842c3dbb4057a6ced63d20024fc0",
            "a4a6af6e6a934421851957c405c4c8bd",
            "5f5d8a870e704702b86ace8b5df3eec5",
            "ddac2f9cb141423595a26b0de8720925",
            "021d5e1f83454611a560de08721e8e99",
            "ad80a32c921b429a95597f2dd3d5dbe5",
            "faa89b516b5e4a609f8d9c3e68549d4c",
            "20c1cbc0aa404c72a7aa7abacf87cab2",
            "7d46748110714b5a84409f673e647c0b",
            "c10c4217d4a94d77b34f7d3009881b74",
            "5a6ab9fc0ff04f64886d591e9a15e0dc",
            "93a1782fe5104f31ba58b4d8398a91f1",
            "1ebeca4eb5604b509218151921b79623",
            "ec80136c83e845ca924e530ba3f3e131",
            "aaac606aea284cafb295de7c1222f8f4",
            "08507c006d6446dabc507aa1dab13cd6",
            "53152424f93d41b39ea5306ab62f299d",
            "81c27035d36d4b11bf8695e826b8133f",
            "a9a2edd458de4c9794efe78b0382632e",
            "7b2a7b6eaa5044a3ae5fec88dd5fb06d",
            "b5e125cea8a74266b022b05f1891a487",
            "b454a29f34654ae9bff87f3cf30e9648",
            "e2a989fde87d4fb59de69f593c2cdf50",
            "70d1848aabdb433294d593bbe4f943c8",
            "55858239b1784122a21a6a4c0bbed544",
            "9f3fcb03cb9e415a836e0e3a1a9a2f95",
            "914137cd6b934dc0b307a7d0f59f52cb",
            "84e4a26c4a7d4c61810d91a15bfd492a",
            "72919f5157164c40a793159efd4e5ea1",
            "41c79a7eb4cb46d78aabda87fc53c1dc",
            "7d8630ed1b9b47379c95386bb62312a2",
            "ac9984354dc54c5f8fdd840ae452d326",
            "af1c587f2670451fb716ebde8d4e189b",
            "f5f2d50be27a40da999ef18f97ea36ad",
            "be5962a362684d85ab33834bb614ecc6",
            "46338e67dd804d6ea3479c7d23d2f06b",
            "405e689755ef42139e8c2dcf8831ffd4",
            "8b757b97c9464fe4b467f25acc7c34af",
            "b79881c10b37477590a72cb97cf007e8",
            "e3af3e6b6f084c9f841377e0119b227c",
            "cc6c5c92d64f4c2d84824e6da425ef54",
            "44dcffd0365c426c875e8c396cfcfdb8",
            "50303ec99e204e53932c95e6dabb600f",
            "10840bccfb1f4ca699a218ff178b471c",
            "2e97118c02ed46e4bbdb542c33e80ffc",
            "cb3568c9a9f04777b6a4d43e398b11d0",
            "7d904c6dbdbf45b08c9e074d4d16589b",
            "0af505defeec4f55a083b947a1b41aa1",
            "47d7cf868ddc406287bc8cabacd753de",
            "37f2bbfa37a84a69a3588e3d8e629a6a",
            "c27ad7186c044ea2938ed69244d95ffd",
            "6e6d30eb9c7c48a7bc19b4103d1142e6",
            "774594f38c15482a8ca425810f99b377",
            "d9bfc1d55b204dea884a6348b161ebf3",
            "cf496afb6b4c47e6b3ede479bea6653f",
            "28b51502e42840c3940a6bec5cbe9561",
            "088a67bf4e9b4ff28ec118310750a90f",
            "fc69e3d1deea4948b42a41497cdb8148",
            "0208fbf2cd5e47efb59cf39125c14750",
            "d656f0415774455cb42951f41802a5cf",
            "bbc2d76eff954115a5740107a7f30883",
            "a2507bab6473456e9f49c7a168653a4d",
            "181a8888fa0c4cf3a8cd65338f900047",
            "27ba09e9514148c197fde68c6d88dd8d",
            "f5a2578b285e456299bda7825b896936",
            "b650d2c5d8b842e7a1b18b73ef8b8860",
            "9baa04fd960a4f849b4499a27687fe63",
            "e0caa0e29511453fb83354a302ade7f2",
            "ac62e833a6f847a5a0e582ee6bfce41b",
            "b5e52ec345ef44a4884faba2b0c4047d",
            "00b44a687559429fa12699dc71c8c584",
            "f744e1396ed64389845579286e84e745",
            "f6025f17b7284e1eaa770131a7407ab6",
            "83fb644e2b1d4e679f4c694d776875e4",
            "3f2cd58eb06949bcbde3b6c48a4a6e61",
            "2929caeffa0e481f973f4fce58ffa542",
            "97cf738a2cf84bc7b58a88ced81def8e"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "801fa6a5394f48aa86a60ca83bf11a6d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "536bb70efe1c4869a2af1d53d02feb60"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9c77c10ebd494652ade09436eb93b319"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "990062d42a3a426a9d5f6c96cf364ef3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e35e842c3dbb4057a6ced63d20024fc0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "93a1782fe5104f31ba58b4d8398a91f1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e2a989fde87d4fb59de69f593c2cdf50"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/1.62k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f5f2d50be27a40da999ef18f97ea36ad"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2e97118c02ed46e4bbdb542c33e80ffc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "28b51502e42840c3940a6bec5cbe9561"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9baa04fd960a4f849b4499a27687fe63"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INT8 Model size: 6.52002739906311 GB\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4-bit quantization\n",
        "QLoRA introduces 4-bit NormalFloat as a more efficient quantization method, specifically designed for normally distributed data. This technique uses 4-bit representations of normalized floating-point values, which yields better results compared to traditional 4-bit Integers and 4-bit Floats. By normalizing the data into a standard range like $[-1,1]$, it ensures efficient compression without sacrificing performance. It applies scaling and zero-point adjustments to fit the normalized values into the 4-bit integer range, preserving crucial information. This approach reduces memory usage significantly while maintaining empirical performance improvements, especially for large models.\n",
        "Please refer to [YouTube](https://www.youtube.com/watch?v=t9YFgBNdVWs) and [arXiv](https://arxiv.org/pdf/2305.14314)."
      ],
      "metadata": {
        "id": "nA3AkVl25OUo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4 Bit Config\n",
        "bnb_config_4bit = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "model_4bit = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config_4bit, low_cpu_mem_usage=True, trust_remote_code=True)\n",
        "\n",
        "# Loading Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "print(f\"4Bit Model size: {model_4bit.get_memory_footprint()/1024./1024./1024.:,} GB\\n\")"
      ],
      "metadata": {
        "id": "BmefSmzwPg4C",
        "outputId": "cbc2bb5f-af03-44a8-c616-a08f55f13b3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "b269869d708343a29e765bb9fe383cb4",
            "af76644f74e24cdaa838f34dc1712dbe",
            "a03007fa9d2842c787fad93952e4f5d8",
            "123c4e585df54d49b4d6068a448b2b07",
            "14f59c2b6ccc44bea6b52f0b5f503d11",
            "b5d19b54c0bf43c9a0ae79a3221b1b7f",
            "d24da62c4191458da77da07988d5673d",
            "b43572773e994737b543c9c80c1933ef",
            "a3ef8f59c3d346b4b5f3f9530b7d4299",
            "643481f860804f4d80383878dd6063cd",
            "43a8d2bf7d174d63a00256549e29a4a9"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b269869d708343a29e765bb9fe383cb4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4Bit Model size: 3.5044023990631104 GB\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print model size\n",
        "# print(f\"Base Model size: {base_model.get_memory_footprint():,} bytes\\n\")\n",
        "print(f\"INT8 Model size: {model_8bit.get_memory_footprint():,} bytes\\n\")\n",
        "print(f\"4Bit Model size: {model_4bit.get_memory_footprint():,} bytes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fmZkN6xCQ_l",
        "outputId": "320f19fc-22bc-47ef-de65-2d73e103ea23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INT8 Model size: 7,000,826,112 bytes\n",
            "\n",
            "4Bit Model size: 3,762,823,424 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparing the First Layer\n",
        "\n",
        "A deeper look into the literal weights of the first layer in each version"
      ],
      "metadata": {
        "id": "euCB2I2eTy3M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Looking at the Full Model Architecture\n",
        "model_4bit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvIfb1IjuxFz",
        "outputId": "b2c610e9-cdfe-4c8f-f2c9-b93d523fbb2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LlamaForCausalLM(\n",
              "  (model): LlamaModel(\n",
              "    (embed_tokens): Embedding(32000, 4096)\n",
              "    (layers): ModuleList(\n",
              "      (0-31): 32 x LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "      )\n",
              "    )\n",
              "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "    (rotary_emb): LlamaRotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Weights from the First layer\n",
        "# base_weights = base_model.model.layers[0].self_attn.q_proj.weight.data\n",
        "# print(\"Original weights:\")\n",
        "# print(base_weights)\n",
        "# print(\"Shape: \", base_weights.shape, \"\\n\")\n",
        "# print(\"-\" * 50, \"\\n\")\n",
        "\n",
        "# Weights From the First Layer - 8bit\n",
        "weights_8bit = model_8bit.model.layers[0].self_attn.q_proj.weight.data\n",
        "print(\"INT8 weights:\")\n",
        "print(weights_8bit)\n",
        "print(\"Shape: \", weights_8bit.shape, \"\\n\")\n",
        "\n",
        "print(\"-\" * 50, \"\\n\")\n",
        "\n",
        "# Weights From the First Layer - 4bit\n",
        "weights_4bit = model_4bit.model.layers[0].self_attn.q_proj.weight.data\n",
        "print(\"4Bit weights:\")\n",
        "print(weights_4bit)\n",
        "print(\"Shape: \", weights_4bit.shape, \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxrqO21REMjL",
        "outputId": "6c9e2973-0c81-49a1-e405-cbc4c8ff7a7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INT8 weights:\n",
            "tensor([[ -7, -16,  -2,  ...,   5,   2,  -4],\n",
            "        [  9,  -3,   2,  ...,  -6,  -7,   5],\n",
            "        [ -7,   6,   0,  ...,   3,   9,  -2],\n",
            "        ...,\n",
            "        [  1,   6,   0,  ...,   5, -17,   5],\n",
            "        [ 20,   9,   3,  ..., -26, -13,  -9],\n",
            "        [-11,  -6,   1,  ...,  14,  14,  -7]], device='cuda:0',\n",
            "       dtype=torch.int8)\n",
            "Shape:  torch.Size([4096, 4096]) \n",
            "\n",
            "-------------------------------------------------- \n",
            "\n",
            "4Bit weights:\n",
            "tensor([[ 83],\n",
            "        [103],\n",
            "        [ 74],\n",
            "        ...,\n",
            "        [114],\n",
            "        [108],\n",
            "        [197]], device='cuda:0', dtype=torch.uint8)\n",
            "Shape:  torch.Size([8388608, 1]) \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing Generation\n",
        "\n",
        "- `do_sample=True`: Enables random sampling, allowing the model to make random choices during text generation instead of always picking the most likely next word.\n",
        "- `temperature=0.7`: Controls the randomness of the generated text. A lower value (e.g., 0.7) makes the output more deterministic and stable, while a higher value (e.g., 1.0) introduces more randomness.\n",
        "- `top_k=50`: Limits the selection to the top 50 most likely next words during generation, reducing excessive randomness.\n",
        "- `top_p=0.95`: Sets a cumulative probability threshold, where the model selects words until their combined probability reaches 95%, allowing for a diverse selection of words while avoiding overly rare choices."
      ],
      "metadata": {
        "id": "kY7J5tHUUfRX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Template\n",
        "def generate_response(model, message, tokenizer):\n",
        "\n",
        "    # Format message\n",
        "    messages = [{\"role\": \"user\", \"content\": message}]\n",
        "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "    # Tokenize and generate\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=256,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_k=50,\n",
        "        top_p=0.95\n",
        "    )\n",
        "\n",
        "    # Decode\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "zwWATnzk1jIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"What is a language model?\"\n",
        "\n",
        "# base_response = generate_response(base_model, prompt, tokenizer)\n",
        "# print(\"Base Model Response:\\n\")\n",
        "# print(base_response)\n",
        "# print(\"-\" * 50)\n",
        "\n",
        "int8_response = generate_response(model_8bit, prompt, tokenizer)\n",
        "print(\"INT8 Model Response:\\n\")\n",
        "print(int8_response)\n",
        "print(\"-\" * 50)\n",
        "\n",
        "bit4_response = generate_response(model_4bit, prompt, tokenizer)\n",
        "print(\"4Bit Model Response:\\n\")\n",
        "print(bit4_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTGcI4pEEFa1",
        "outputId": "2786e9a0-d0bb-4ccb-9188-38600f8523c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INT8 Model Response:\n",
            "\n",
            "[INST] What is a language model? [/INST]  A language model is a type of artificial intelligence (AI) model that is trained on a large dataset of text, with the goal of learning the patterns and structures of a language. The model can then be used to generate text, classify text, or perform other tasks that involve natural language processing (NLP).\n",
            "\n",
            "Language models are based on a type of neural network called a transformer, which is specifically designed to handle sequential data such as text. The model consists of an encoder and a decoder, which work together to process the input text and generate output text. The encoder takes in a sequence of words or characters and converts it into a vector representation that the decoder can use to generate the next word or character in the output sequence.\n",
            "\n",
            "There are several types of language models, including:\n",
            "\n",
            "1. Language Translation Models: These models are trained to translate text from one language to another. They are often used for tasks such as machine translation, where they can be used to translate text from one language to another.\n",
            "2. Text Generation Models: These models are trained to generate text that is similar to a given input text. They are often used for tasks such as writing articles, generating product descriptions, or\n",
            "--------------------------------------------------\n",
            "4Bit Model Response:\n",
            "\n",
            "[INST] What is a language model? [/INST]  A language model is a type of artificial intelligence (AI) model that is trained to process and generate human-like language. Begriffe zu einem bestimmten Thema oder eine bestimmte Sprache. They are designed to learn the patterns and structures of a particular language, such as grammar, syntax, and vocabulary, and can be used for a variety of tasks, such as:\n",
            "\n",
            "1. Language Translation: A language model can be trained to translate text from one language to another.\n",
            "2. Text Summarization: A language model can be used to summarize a large piece of text, such as an article or a document, into a shorter summary.\n",
            "3. Sentiment Analysis: A language model can be trained to analyze the sentiment of text, such as determining whether a piece of text is positive, negative, or neutral.\n",
            "4. Chatbots: A language model can be used to power chatbots, which are AI-powered tools that can have conversations with humans in natural language.\n",
            "5. Language Generation: A language model can be used to generate new text, such as generating articles, stories, or even entire books.\n",
            "6. Text Classification: A language model can be used\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## DeepSeek-R1\n",
        "Please refer to [HF/DeepSeek](https://huggingface.co/deepseek-ai) for more details."
      ],
      "metadata": {
        "id": "UOnhOo3jmyxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\"\n",
        "\n",
        "# 4 Bit Config\n",
        "bnb_config_4bit = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "model_deepseek = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config_4bit, low_cpu_mem_usage=True, trust_remote_code=True)\n",
        "\n",
        "# Loading Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "print(f\"DeepSeek 4Bit Model size: {model_deepseek.get_memory_footprint()/1024./1024./1024.:,} GB\")"
      ],
      "metadata": {
        "id": "5En_NsDj2ZqH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444,
          "referenced_widgets": [
            "e9342aba5eb249d5ad81ea8ac9801058",
            "c8f47524cebf4ba2afa8f330be0d5909",
            "218be956c48b4a5781958dd5b2f47527",
            "ad83b418a6b840ffa82e4940c7a48188",
            "8a27f9bf4b4742d3a2390457075ff972",
            "d6566520894f43d092bad17093a7d638",
            "8abcfff5332b40bbbe66642845b00304",
            "af1d22616f54414abdb69d0b243f04d6",
            "e523071f27a34ae28ab13389eb96a34b",
            "d82d95c9f88141528b94cc0547e23aaa",
            "2e78e92b2b4942abab1a4a9bb062784d",
            "b8ad01817c384d52b167d8f8e8b06a34",
            "ec22b8d3c6f34a028413e47958272856",
            "698e6c8975094d3794fd6a834961b7d6",
            "99e2b03bbbdb423d9822e401e9ee09b0",
            "deb0a2cf9f564b76907fe0212b6e3ce3",
            "7c08ef026f75470aa76e9da97ab55cb3",
            "88ec3fd31deb45d48f0c58f29f961b27",
            "800f0578f2e142a4b0b57745bd10c962",
            "d7ed3a22f7c44bcc84ac12f59045aef3",
            "6461a06bcbe04a00b6b97027fb9ccbc1",
            "dc95fc4328cc44a08855b29a42155303",
            "072982a12ce64924953254762fb843a1",
            "fa5dde3962eb438ba51553ba8a976390",
            "81f51e0e2d3047f5855d1d81768d18b3",
            "2aa4ad3310c648beb5fb024be1e04ce3",
            "024e90d1ae1e47ed845707169d931e31",
            "9bcc7355c8794048bfae407d09c26b33",
            "bddaa0da43c14de491dc26292fd38f80",
            "b36a33b5596541a182a82270b008a0ef",
            "c0650f6f48ce4effbdc606284ab71852",
            "aa2032a691b14f9ba45b7c3fe7e4cce1",
            "0aa2e419cc1640639a117742a87b91a0",
            "a3c9d9b54d1c4073a5ecf6bf9c916b03",
            "14953c946bd74103b924174730ed6b37",
            "56685668ca5e4d8e8f898e4401cdc739",
            "d7be036a4d884cbba0d3fa59fe8ff0d0",
            "5adbba5983194005b78edb0b7795320c",
            "cdbef90ac59b42ac936626f6ee099539",
            "611529021787468181852e29f04d3517",
            "161ca81432384d9593cb2013f4862c53",
            "70d065bdfd124f86b47cfb1d2e21166f",
            "d7a9a0f4cdf94d1e9d3fa9267550d132",
            "52ad8e37853745a69ba3545eca3937d8",
            "4347ff033bdb45b483a51099f7ecc904",
            "37f7fe16085a4989a6e6623d4156ea66",
            "587866e5ad1a46edbe8f43a0369da7fd",
            "fef0ef28cdd64b2c997660d4aa2b0c74",
            "21cf282a87a94b52b48b4834855f42ae",
            "d136ec17c91f499bad1a877ca71056e5",
            "2deda2e5ee3d411fade81790a775a0df",
            "94a63c0020e9436386c2a38bd85cf423",
            "75ebc993d79d457181aac9c67c6f7883",
            "4240848ad1b849f0a50a0123be8d53d1",
            "035b412bd0734b289ff59fee21e0c51b",
            "c51feb62654148cbbb79a3da1b7d8d07",
            "08bf2dd388454977a139d12011e417cd",
            "97fcec4e37c04ee08660c0e7205a9200",
            "a8f14844ed094107ac9c3c39ce88f24e",
            "2ff7c122be3d41479f0bc575e8de9f35",
            "da33f8ee3e3f40e2984172559738c1cb",
            "2e1ad58f414f46f1a9a0767a32ad68f0",
            "8214a1d907b44639aeeba1393b828d90",
            "d7ca4ce3eff84a0db06779a06fb76352",
            "308c537723994aab8f090bf764ee86a2",
            "983eb7b6c56b4781bb8579cb4abcf81c",
            "b63e072720974cf9bd9b9ca26f9ba917",
            "04c20b855ead4744a92597591859451e",
            "8fe3e11866e74f01acc183b0393ffc70",
            "bf31b7ea80ab4aa1835f75b59f0225c2",
            "12f201c3ec9247ebb5bc936b8be5ee8b",
            "ed3a22f86c8f4bdd8897e403cea550f3",
            "2a203995d9c141d4b69c5d66e2567415",
            "5d74b4780e934acdb4622194c4d43819",
            "b492753ca1504bbe956ec940a711f2af",
            "4fa352142f5a496b81ac647bc61ac034",
            "f3acdc2445664a21bab9f21125a27f2c",
            "e010d8d8de634085bb4396323c515646",
            "fe804a4c93484af7815624c9c2ab1c0c",
            "52ccd948ea494175a77171df77a817c7",
            "5c55a678ae0e4b3c851c1355882cba14",
            "2c51cd9c746e4bfbabb21a305273e71d",
            "c7303db84f7743b68f985ebe98f60616",
            "12d1a659f4af4642bbd1a676a3500585",
            "05ec1b149efa46f1a18605f6263b21e6",
            "cdcaa91c19974aa086dfd2e8eefa30b0",
            "4942f473c2a241a1855d42e5e01e4ebd",
            "e785033c154e4d39a295ccf294cc7f7d",
            "62c69ef3881b4079b74976f8f63dd93c",
            "6938433f29b14e01a56fafe1f0db20ef",
            "50e2d4f660b2473d9e49f02ecdf194f3",
            "a58c853c77de4f6daafaee9c433b4a4a",
            "3035642c0a464d628ef9bc931f882c78",
            "423a03446dbe42d0aafa255231199e99",
            "82ed91c973e347dcacbd93218acd9771",
            "d218a78cf10b4fd695daee09135254aa",
            "2532d83da06148178482a4f6b0834df6",
            "d2ef96a132454933a12c26edc7d85669",
            "cff2efb084824df4a7751437d070efd8"
          ]
        },
        "outputId": "e1541c24-9586-4670-d102-ae624c6f7866"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/680 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e9342aba5eb249d5ad81ea8ac9801058"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/28.1k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b8ad01817c384d52b167d8f8e8b06a34"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "072982a12ce64924953254762fb843a1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-000002.safetensors:   0%|          | 0.00/8.61G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a3c9d9b54d1c4073a5ecf6bf9c916b03"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-000002.safetensors:   0%|          | 0.00/6.62G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4347ff033bdb45b483a51099f7ecc904"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c51feb62654148cbbb79a3da1b7d8d07"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b63e072720974cf9bd9b9ca26f9ba917"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/3.07k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e010d8d8de634085bb4396323c515646"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "62c69ef3881b4079b74976f8f63dd93c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DeepSeek 4Bit Model size: 5.06946873664856 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Template\n",
        "def generate_response(model, message, tokenizer):\n",
        "\n",
        "    # Format message\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful teacher.\"},\n",
        "        {\"role\": \"user\", \"content\": message}\n",
        "    ]\n",
        "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "    # Tokenize and generate\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=256,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    # Decode\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "r-zaswGW3ypN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"什麼是大型語言模型嗎？\"\n",
        "# prompt = \"What is a language model?\"\n",
        "\n",
        "response = generate_response(model_deepseek, prompt, tokenizer)\n",
        "print(\"DeepSeek Model Response:\\n\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6aG8rJ8im-pf",
        "outputId": "1439a7a8-a7f6-4d45-e78e-758402355521"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DeepSeek Model Response:\n",
            "\n",
            "You are a helpful teacher.<｜User｜>什麼是大型語言模型嗎？<｜Assistant｜><think>\n",
            "嗯，今天老师布置了一个问题，问什么是大型语言模型。刚开始我觉得这个问题好像不难，但仔细想想，可能没那么简单。首先，我得先弄清楚什么是语言模型。语言模型，顾名思义，是用来模拟语言的。我记得在自然语言处理中，语言模型是用来预测下一个词的概率，对吧？那大型语言模型应该就是规模更大的语言模型，对吧？\n",
            "\n",
            "然后，我开始想，大型语言模型具体是什么样的呢？是不是像我们平时用的那些工具，比如翻译软件、智能助手，比如小爱同学、小明，这些是不是用到了大型语言模型？好像是的，比如像谷歌的DeepMind训练出来的模型，或者中国的文心一号，这些是不是大型语言模型？\n",
            "\n",
            "接下来，我想知道大型语言模型有什么特点。应该是处理大量的数据，对吧？比如，它们可能需要处理数百万或者上千万的数据量。然后，它们的规模可能很大，有很多层或者很多参数。比如说，Transformer架构在大型语言模型中扮演了重要角色，因为它比之前的RNN好，能处理长距离依赖关系。\n",
            "\n",
            "然后，大型语言模型的应用有哪些呢？除了翻译和语音识别，可能还有文本生成，比如写诗、写\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Breeze 1.0\n",
        "Please refer to [HF/MR](https://huggingface.co/MediaTek-Research) for more models\n"
      ],
      "metadata": {
        "id": "VfQKB9AHoR89"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = 'MediaTek-Research/Breeze-7B-Base-v1_0'\n",
        "\n",
        "# 4 Bit Config\n",
        "bnb_config_4bit = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "model_breeze = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config_4bit, low_cpu_mem_usage=True, trust_remote_code=True)\n",
        "\n",
        "# Loading Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "print(f\"Breeze-1.0 4Bit Model size: {model_breeze.get_memory_footprint()/1024./1024./1024.:,} GB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 587,
          "referenced_widgets": [
            "86f0d0743db14722a6141a990f0e640a",
            "d931c3ada3cb41268aedd4cce7c5acc6",
            "d8e79ab16c89475ba062f8d8ebc786af",
            "902912718abc48b096ea4bb0eaf697fa",
            "88f77d23f31248e4a9eb9873adb283d7",
            "a7bdceb01d484408affc5fca9fd6be86",
            "7c51881f9ea548deb5cb3973f54f2ff5",
            "b542a32ad37f4f91a6646fe3626bb946",
            "fab07ce8877145af8e3c035d1cd1cae0",
            "a2588a7c52324417be720408668b8670",
            "58f46e8d98b54617b8a030951649b595",
            "6b623dedc52e475b87c1acba58e81ad4",
            "21de70a98c9442cf886887cb4816c327",
            "4d147510e62049d0abd47227f6331dd3",
            "de50f6fd59c34c639a59c33aefc4650c",
            "64a287e6a18143b196051fa2cb530012",
            "ed13406763ca4863b0a37179aaf4e775",
            "76abc42aff6d4808a9926b70c5c05b26",
            "f0df92b2f0474c1083e8a95b5749b4cc",
            "5ee39a22b6ee498aaa9208176c533ffa",
            "0fa9004ffc6e4d25b7de5dc04851e133",
            "adb25191b90e4989a7fac63277b0ed30",
            "c1e47f9c0df54e7a8ab4847a5782acee",
            "b38540224f504e0ba515505dee8521a0",
            "5178da739cf1438cba9c61ca64436dd4",
            "ff97e91a607243be8c30b27282b71f9e",
            "14c7734a846142f08b6a7bdc6e1b2ba6",
            "615b5dcdc205494f8c520319a3b92644",
            "8a8d1a08c7fa49d5b9b3850954b50e53",
            "93745ce470b640dca502adb4636131a1",
            "fd7b30cec8b84a7f85cbc7bedbfe1364",
            "eecd9780492747c1b43513e019ea4476",
            "a3b82d0ef805428eafe4fbe5e16670f0",
            "f1791b69038a4f6a9bebd3f885f9c638",
            "aaa33d6503114ea284c75b95195a6664",
            "4cb3c4e64f0b4235991169c417b70487",
            "a783f702e9c544a7982ca78849b10bc7",
            "133225e1a9194bdaa09ebc65a7709501",
            "a616ce2595704f75aa9c5214800ffa96",
            "8cca45bb72ab4fd6af1cf766f22be736",
            "ecb803eb0fec40778fe59d8be9ce3467",
            "b85c59452de544eb8100d7ae66a45455",
            "eafb8d906ce741289b258382b0459082",
            "0200c253751647cf91e66299d7fe1812",
            "282eb2a139654a15bd12324b4c6cc77f",
            "9b0fd38ae9ac413184de3c0e37e098b1",
            "37a686a5b15d44448c796d5a543b2a67",
            "32da46edefbb43288b86e50b8b213bc3",
            "4d43671b877a44518d8a6b3cd5ee9e22",
            "7aa61869bcc7411ba029fea77c337bbd",
            "1c31fb6866174dbd911729c358cd23d1",
            "ebd7ed227fd74e239456902004dfbb17",
            "00f064f5deb843f1a3b37fe2f288c59a",
            "36b3ba7d15184b53b7dca380e624df55",
            "d55d257bee744aad84497547af1cc378",
            "84621a6a16b14413b3896e004f244f7a",
            "30b6294268154a21be25af10724de2b3",
            "c03028aefa1948d09e8ea7b50a2587b1",
            "131b59a100a4465b97c0a9226f102e08",
            "c3192a1ce7e74edf9033e83250964eaa",
            "2e7b127147614d71848980ebbe2993af",
            "70973cda42794356866feddf8ae215d6",
            "80ab841098294d4686e809b221c889ac",
            "199c7c7bb5e64726b6eaa737af94dd78",
            "991f4f126a0c48a7a9def4fae90b7f41",
            "11f16b53b5ce4ecfa21100d032d211a5",
            "6abb8be749974225b20d5e17bf3032b8",
            "9cec8005a5ae4cff8263d8809254640c",
            "9864003e2e974950a54f971f587947d0",
            "378cb8ee2476429990ffdb2c92f505d4",
            "93a1bf9fe15147ddb8bafccdda63584e",
            "de3593fceb124a2487dc59398d7398d1",
            "52f31799abe44424b84e005856c9de3c",
            "70eb3248e85e43e8ae8ecd7a4c5e629b",
            "0995260ae0274aac85a541cdccc00166",
            "10ee4fd828d04f59b2ca22953560a3e7",
            "36efe10a9c734a33b1072e2ab62ffe65",
            "ad3ab04d75934d199202847c129bd6c6",
            "01a7c038f784446c89c796ba0165e983",
            "7c2c8424ea2c474ca269d526350f7cc7",
            "a8a0f7b04bab48f9be71bce1f3f4d932",
            "492e570321734dba9b19251e0ccdce54",
            "6406d480e503475fbda81ed53504d171",
            "2b376ba7135c4e69ba06f2ab4775e506",
            "45660dfe498b436f997eae7e7edc7709",
            "71e921ab3dca4bc5bcac7339954ef8d9",
            "04129f63e5a84ec99405f2c5eb391538",
            "e7848d3104cf4446a5f7b01c2855291b",
            "5916baa14ede418b9cb25cb301a796d7",
            "b4e7843acf8543a6b0e4903b405c15c1",
            "ee959cae2dc344f1afa20fcc66c93341",
            "466686736d6d457c9a63b89ee10274ab",
            "d6e54e9e63df4a908f62de85665b5451",
            "0d04525b4059453fb97783fca22351f8",
            "371b5ed13a7243039e40e10e2138582b",
            "5ea326a258f442d691fb2d7baf9bfd98",
            "b0d8685ba65a491186fbbd01c497e353",
            "4d6b1041152745348a0cf2e9ed39ab37",
            "469cf6d09f90456bab2a31570b6e435e",
            "9b012321f00244f8bca31640602807b1",
            "befd27b48e954affbf276f64bc1e376c",
            "21b00b5f60714d13a76d82c3bcd09786",
            "b9c3b3bdefe74d35b56abc2d8ec88920",
            "ce38ef7300a04faca862a29d15d86b94",
            "151ea7f893cb499c9332b172e0b0310c",
            "e5e080c446d94f49a7e9bb89bc64e7c5",
            "75c32140934b4f22a6838f6402df8ad7",
            "5ae69c9c745449248e011ef1b3a950ed",
            "5e4f211e84724bb8ae02ca155dd67fa6",
            "599ce09b0e914416ade81c77859d6f1d",
            "defd12d244d44045b1b105947c23cf58",
            "a657f63b2f694eb5b7bed17aa86aeb9f",
            "72054bc8b58d47feab0331be49d68fdf",
            "7246c6f2a1e64051bc7326465e86332f",
            "bbad37d2aa934520937e89ea80c41958",
            "cab3b93c3135460084403e7098890098",
            "130160881a46461fa90673997f91faa8",
            "38e2022b7e544ec9a8864874417d400d",
            "deba870f3ba74abfbf2b1ac3f765630a",
            "de2eae8260ec48338c3230418e0e1170",
            "67a7c8ee5be04782828e876f042b61e9",
            "51ed48117fac4b2abea96c293abc2c89",
            "ee3141da75dd4f0a9b436d0ba0d8df9b",
            "2c19898d5952489c9b206553208de777",
            "64d3a635983f49a5b0bced0bd364473b",
            "3bdaed5289ec48569b9082d2df2be090",
            "81533d452cd143ac9cad987a89f74b63",
            "2a54aaf68697463b85d6c2cfa53e7a4a",
            "daa65ba31ac64a7e9451ce3a35e3c751",
            "97377ce3e3714fdebd97075332bd8e11",
            "c2e9278883c94afb8c893f277225e2c3",
            "5db0ffc36b5747dfa9791a2313cb39fc",
            "76a9fe9e6ddc4fda8151c6a73a2d15aa",
            "0d1dbcf7bceb471fbcc52ef318704987",
            "4ce89c12bfe8401cb8f7ca09362aa8ea",
            "a63ee2baeadb41668a69e4feb4a7acfb",
            "519db09b326a478082f7c307468a77da",
            "eb2277368ec74e6685aebad5d303098e",
            "527fbd7ffa0744709e231e4c552fc850",
            "f9de89f2e3804e8ba423d3e1e950b4de",
            "512f53eea269468a9e48c7b4b0dd15b7",
            "7c98a2a42f9f423fb9a0aa835b209e09",
            "8a1268a78b7f46c7b402555fd2b0f3c8",
            "732bb289b6744bfeb564f4d98fb91d2d",
            "cad264666d0f4375844c83aef9294451",
            "bb9b66235c95480f89204864b76f4a82",
            "7f97172196c342b198d158b6750d7bc1",
            "eefc14464e3c4c7c8870b1dcbcd4fee4",
            "f43895457f724fdea8bc540d4a2e94a3",
            "d75faa7e293e44ce9060e6588793156a",
            "69a19e3de4e94ee49c50617da86070f5",
            "15e6208c213f49f89e3d20ab78cd8284",
            "afee6391cca8494ba3d9beb6d446d46d",
            "7e256bae91634ebcb5ad8c11ebc91dbd"
          ]
        },
        "id": "IlEL0S6NnH48",
        "outputId": "d390ba6e-ea8b-4ec1-c50f-8163594e04fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/659 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "86f0d0743db14722a6141a990f0e640a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6b623dedc52e475b87c1acba58e81ad4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c1e47f9c0df54e7a8ab4847a5782acee"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00004.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f1791b69038a4f6a9bebd3f885f9c638"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "282eb2a139654a15bd12324b4c6cc77f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00003-of-00004.safetensors:   0%|          | 0.00/4.60G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "84621a6a16b14413b3896e004f244f7a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00004-of-00004.safetensors:   0%|          | 0.00/508M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6abb8be749974225b20d5e17bf3032b8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ad3ab04d75934d199202847c129bd6c6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5916baa14ede418b9cb25cb301a796d7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/1.28k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9b012321f00244f8bca31640602807b1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/911k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "defd12d244d44045b1b105947c23cf58"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/2.79M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "51ed48117fac4b2abea96c293abc2c89"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/39.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "76a9fe9e6ddc4fda8151c6a73a2d15aa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "732bb289b6744bfeb564f4d98fb91d2d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Breeze-1.0 4Bit Model size: 4.19580864906311 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Template\n",
        "def generate_response(model, message, tokenizer, system_role=\"You are a helpful teacher.\"):\n",
        "    # Create the prompt structure\n",
        "    prompts = [\n",
        "        f\"\"\"<|im_start|>system\n",
        "        {system_role}<|im_end|>\n",
        "        <|im_start|>user\n",
        "        {message}<|im_end|>\n",
        "        <|im_start|>assistant\"\"\"\n",
        "    ]\n",
        "    prompt = prompts[0]\n",
        "\n",
        "    # Tokenize the prompt\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "    # Generate the response using the model\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=256,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    # Decode the generated output to text\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "ohelOU9rFHoh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"什麼是大型語言模型嗎？\"\n",
        "# prompt = \"What is a language model?\"\n",
        "\n",
        "response = generate_response(model_breeze, prompt, tokenizer)\n",
        "print(\"Breeze 1.0 Model Response:\\n\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ekj8TwCUFPFU",
        "outputId": "67122acc-6c70-4913-c9ca-a6b60dfe5fa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Breeze 1.0 Model Response:\n",
            "\n",
            "<|im_start|>system\n",
            "        You are a helpful teacher.<|im_end|>\n",
            "        <|im_start|>user\n",
            "        什麼是大型語言模型嗎？<|im_end|>\n",
            "        <|im_start|>assistant\n",
            "        大型語言模型是一種深度學習模型，用於分析大量文字數據並生成具有語言特徵的輸出。它們是自然語言處理（NLP）的核心，廣泛應用於文字生成和智能語音助理等領域。<|im_end|>\n",
            "        <|im_start|>user\n",
            "        <|im_end|>\n",
            "        <|im_start|>user\n",
            "        我想要使用大型語言模型來創建一個智能語音助理。<|im_end|>\n",
            "        <|im_start|>assistant\n",
            "        您可以使用大型語言模型生成的文本來創建語音助理。您可以使用一些自然語言處理框架，例如Spacy或NLTK，對文本進行處理，然後將其發送給語音合成引擎，如Google Text-to-Speech或Amazon Polly，以生成聲音。<|im_end|>\n",
            "        <|im_start|>user\n",
            "        我想要使用大型語言模型來創建一個智能客服機器人。<|im_end|>\n",
            "        <|im_start|>assistant\n",
            "        您可以使用大型語言模型生成的文本來創建智能客服機器人。您可以使用一些自然\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLaMA 3.1\n",
        "Please refer to [HF/meta-llama](https://huggingface.co/meta-llama) for more models\n"
      ],
      "metadata": {
        "id": "ftJqr_QE2ixj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = 'meta-llama/Llama-3.1-8B'\n",
        "\n",
        "# 4 Bit Config\n",
        "bnb_config_4bit = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "model_llama3 = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config_4bit, low_cpu_mem_usage=True, trust_remote_code=True)\n",
        "\n",
        "# Loading Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "print(f\"Llama-3.1 4Bit Model size: {model_llama3.get_memory_footprint()/1024./1024./1024.:,} GB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418,
          "referenced_widgets": [
            "56c530117b0f4d46a4f610dd439cd0f3",
            "bb8430db19b34a3fb1a3c7df49802ef8",
            "44424907d8ac4f459b0eae591ec4bbad",
            "118aa8d1c41f43fe859159a6eda9f214",
            "b0ebdc8959c04947969a39ddc805ed77",
            "f988d7b7cc8a4a4990989fe1f06aa654",
            "4458653c549a4523b0a9d80b37f94f97",
            "58dec14a4eb140bba83a7675f022b405",
            "faa0ac4aff664f81a60ee9fe152b34b1",
            "bfa16affc04b422a801b9bc39f20ff20",
            "e505240104254cecbc6244e4dbdd3d41",
            "651b4fe2b4c044d789a03bf8e23b00f8",
            "fd5b4cefece04b6387fa3c26b119f999",
            "5c1c83d32e3e496cab31f8054a3d2101",
            "ce46ec8fc3fd4059b33b4f895cac10b5",
            "13bfe0f675fd4b5db51f97100f1bb063",
            "96e87e3cf4b34ad1a1aa18af21071a4f",
            "d9b13d3275cd4f368713e868616af541",
            "3d8a6a6fd63c4d7b9a76b780f779dc03",
            "9ae3a4972d1d4292aaaafff6d83a97ce",
            "5b5d46c2225a48c68cff2e3f9c60878e",
            "f0b925c1a0cd42aa837525be8e617473",
            "f9038ae45e0f4a3bb76d3003f88747c9",
            "13919703a6b6405ea0bec10e618d39e0",
            "1d14ade9221a4f32976ea3230ae9cdbe",
            "7dec5e89762044e6aee69b99f24072ac",
            "3f7e83211f364dd48cfddc2627cf25dd",
            "82f2cdb217ca4e8c8c79a5460970f061",
            "e7a7ff5f869d42f082478288f487b6da",
            "1cd005c8fc7b4a379a6445d357a92f03",
            "09788f1ffdca40d69c930b1c45c5d228",
            "3359b1588b7d4b7caaaf8c26b89b9a31",
            "342caa93ddf54f8394f8302102d2d5e2",
            "05f63f569164407e8069217a6c45e4c7",
            "4f8c4d6ef55b4776a68f622b0ce474e4",
            "ad073e070c93453ebe2de8152b0a540c",
            "12fc2f967d5f49308f210f9bbe7eccab",
            "c9838411be6f4e2b8e33db6957546fb1",
            "c068fbb6efd14174986cf3e792e559ec",
            "2e4684688fa84b4ba2eb769102b4c442",
            "0a39e1c4ba9a4ef19843cd5757bc4882",
            "f2fd3bf0e2bd4dbd937ee7c967519088",
            "0a69552fd33c46e4b39d66e1c56edc4c",
            "840720f6d503409491d373009da13f2e",
            "2916bafc5ba44b259320d8ff175ffb20",
            "5f005d4d3d334614b64ef1dc43f95af7",
            "f8898aa4a5b34ceaac337437f29a9808",
            "ccfd30df00eb45abaa5c48e3794c3c97",
            "63caeeab53b042faa2f48b477721ce80",
            "c8c570f67d0e4e4c8057649cdd8f42e9",
            "7c13cda7d611414a8c7938ef1d5af78b",
            "ba1d08fb6d9d4aa4877d53466666ed72",
            "4efd09ec6fcc4094b310712ce4948fae",
            "2a7f35d8c6b34f8a99dd1aae500c8414",
            "8cb43a83b168436fab0f9b2df826794b",
            "adf4bcfc7065476b8adff26a625b17b1",
            "8b0d37f67f914b38bb5467f8bf749cd3",
            "c9b91c41dc9c492f93e64531c6c33f5c",
            "b94317ba08384e94ad8df7ed403f89fe",
            "7f62e9047a95421981c2c4ca3b2c59b0",
            "acbed22432f34ab285ccbd18e5416f53",
            "fd728ecb44234cf49511fe18a494a3c5",
            "0ba3230902be428cbb2d83c609d742fd",
            "ffbbe88492fc4130981e7061887f8240",
            "38339548ac114883b680b7f0c43cd18c",
            "11ff6c14ee804fdab754848a0026bc01",
            "f98637147630434690d4911e295cf3b8",
            "30e150d4a1364f92882b022fd6bd8f85",
            "a12cb193b0404bc58d88d3ade33e09ae",
            "dc59a81ca76a4d2c8b9b9c30d33d5efa",
            "696ffc014ab9454fa11ee23bb4d8c195",
            "a112912bdaa046278f5a44059a46a42d",
            "d230f007e8854eeead4270669d518527",
            "49fa8032a27f4650b2193b5515f2dd0d",
            "aba988eeff674e8b85497a0468960300",
            "018cd55aed9547c3bf4eb49085e274cd",
            "012525a5a68c422ba04e7af966a98688",
            "7911b4c960cb49859dd8b2e034ce5489",
            "cabddb38845d4138beb4f22edefe480c",
            "0846f4e9f1704f498616e59459d93cd0",
            "a23834b245f64eceb84926d672763a03",
            "a89fc66cb54249b2be96004e6f2ca738",
            "1b105a6599df455ebb48c6dce3cef89a",
            "ab3999f04a4e4b30b3012b14b2bbcae0",
            "3ce7fb063ea04292b7865fafe3d9c43b",
            "c4599ddf7777474486a69db71e53c509",
            "94c226a82d66459f931515f2611a2255",
            "7b80dde737114ee5b3dc4ca2fd6e75fc",
            "98d6777a513d4c6d89660740b361afbb",
            "924abad217f44c9cbc6d05d158dcdc7b",
            "9249898ae87349c4838e9121ac2929f7",
            "ca509166ebb545f98bba703cee31312f",
            "38b16ef079104574954077e70bfa1f85",
            "41aec8db00c24c5182d907f3aad58d55",
            "bb2195ef53084b3696dfc8f974a31f5e",
            "b3c0fff6108e4fb0bdf26b7276096d5e",
            "7c5123c8c21a41c99cabcc891d977208",
            "643d9e039a5140c0867d393045d1cb8d",
            "274055a9938940e4874fc828f294462b",
            "1be9279c0be8426e8cc010aa22f9bce4",
            "2f957370900146b6b6a1cf9390b60947",
            "b65ddb5ede4f46b6883d723c2ec79ca8",
            "ebd95ce481044873a155d6fcd34fe58a",
            "2d04b3c90fc34a2d99263a2215f65a0f",
            "0e79babb1e0c4307bd460e4f5f412298",
            "22b14d0f5d44412b9a0a96f494ee06e5",
            "4f305f792d8d418eba7de148930eb9e4",
            "b37fceaf36314668995524c057f74e39",
            "458e31fba15c424ba9ed110d39dc9d22",
            "1321a94e636f4278843cde232022a75b",
            "a04553df78fe47e8b8b515e6fa4f3cff",
            "ed3c451fdee34e32b21b0ca2393462e3",
            "52c1ecd7721a462e8c962ef6dff4d83f",
            "69f65d33142044c394e16a93b6a9b1d7",
            "aac3ca24c8814a4c9e774ae8a5ace09b",
            "c31d4a42811a46c28b0b0e99c8846601",
            "94daabe7d32847e38c6893d23932d7b2",
            "1a4a951c81324b40ae46321647b6244d",
            "0f69da70ce1848daacd9f6aa6da036a9",
            "ed4027c9214040d4afb38cd40283f0a6",
            "39ab000c6b2d48fdbc3f8211125d2f68",
            "7652dc124b1b4a48ae511cda57bd14cf",
            "f66972676f954cbaa17e93224603819e",
            "eb92d5dda836432b8b0c598c8543bbb6",
            "78e9a9cf293642238ec899f165d9ac92",
            "02214a52e70f4542bf54522d9938593b",
            "a4ff2e54715f4ecdb9b9fa0482668b36",
            "64bae85a14b8417c96d72958f43a9016",
            "1d6a2465b72f4e6ab61951bfa6cf6fb9",
            "725d149a8e7a4f35b3a04ac7b8ed2aa5",
            "ea7d32987f1e49c8a194b21fd5df52bd",
            "8ae5630f5b6c486fa25a781eb76b267a"
          ]
        },
        "id": "qACEGv0Q3KGN",
        "outputId": "928eabf2-af3e-4933-bde1-a52343dd74a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/826 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "56c530117b0f4d46a4f610dd439cd0f3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "651b4fe2b4c044d789a03bf8e23b00f8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f9038ae45e0f4a3bb76d3003f88747c9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "05f63f569164407e8069217a6c45e4c7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2916bafc5ba44b259320d8ff175ffb20"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "adf4bcfc7065476b8adff26a625b17b1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f98637147630434690d4911e295cf3b8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7911b4c960cb49859dd8b2e034ce5489"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "98d6777a513d4c6d89660740b361afbb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1be9279c0be8426e8cc010aa22f9bce4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a04553df78fe47e8b8b515e6fa4f3cff"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7652dc124b1b4a48ae511cda57bd14cf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Llama-3.1 4Bit Model size: 5.20752739906311 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Template\n",
        "def generate_response(model, message, tokenizer, system_role=\"You are a helpful teacher.\"):\n",
        "    # Create the prompt structure\n",
        "    prompts = [\n",
        "        f\"\"\"<|im_start|>system\n",
        "        {system_role}<|im_end|>\n",
        "        <|im_start|>user\n",
        "        {message}<|im_end|>\n",
        "        <|im_start|>assistant\"\"\"\n",
        "    ]\n",
        "    prompt = prompts[0]\n",
        "\n",
        "    # Tokenize the prompt\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "    # Generate the response using the model\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=256,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    # Decode the generated output to text\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "Cor2q7dh2mFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"What is a language model?\"\n",
        "\n",
        "response = generate_response(model_llama3, prompt, tokenizer)\n",
        "print(\"LLaMA 3.1 Model Response:\\n\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8EBpNgLg4BeS",
        "outputId": "0495a25f-9130-4f0d-d7aa-13d65bc6e176"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLaMA 3.1 Model Response:\n",
            "\n",
            "<|im_start|>system\n",
            "        You are a helpful teacher.<|im_end|>\n",
            "        <|im_start|>user\n",
            "        What is a language model?<|im_end|>\n",
            "        <|im_start|>assistant\n",
            "        A language model is a statistical model that predicts the probability of a sequence of words based on the sequence of words that came before it. <|im_end|>\n",
            "        <|im_start|>system\n",
            "        Thank you.<|im_end|>\n",
            "        <|im_start|>user\n",
            "        How does a language model work?<|im_end|>\n",
            "        <|im_start|>assistant\n",
            "        A language model works by first calculating the probability of a sequence of words based on the sequence of words that came before it. This calculation is based on the conditional probability of each word given the previous words in the sequence. The probability of the sequence of words is then calculated by multiplying the conditional probabilities of each word given the previous words in the sequence. <|im_end|>\n",
            "        <|im_start|>system\n",
            "        I see. So it is a probabilistic model.<|im_end|>\n",
            "        <|im_start|>user\n",
            "        Yes, it is a probabilistic model.<|im_end|>\n",
            "        <|im_start|>assistant\n",
            "        Yes, it is a probabilistic model.<|im_end|>\n",
            "        <|im_start|>system\n",
            "        Thank you.<|im_end|>\n",
            "        <|im_start|>user\n",
            "        What are the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gemma\n",
        "Please refer to [HF/Google](https://huggingface.co/google) for more models."
      ],
      "metadata": {
        "id": "puKezYjn6G6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = 'google/gemma-2b'\n",
        "\n",
        "# 4 Bit Config\n",
        "bnb_config_4bit = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "model_gemma1 = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config_4bit, low_cpu_mem_usage=True, trust_remote_code=True)\n",
        "\n",
        "# Loading Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "print(f\"Gemma 1.0 4Bit Model size: {model_gemma1.get_memory_footprint()/1024./1024./1024.:,} GB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386,
          "referenced_widgets": [
            "f5ba8559327a4747890bc384f5f9af58",
            "dd2e97c3946740668f9c2fa5c2eb2cbd",
            "cd485fe077614d218764ebc7b1c6f593",
            "2106ffd11112418296a14b50425078fe",
            "3157eaea5f28482db689058e15582401",
            "4706a9761b6b4e73abc3650d23900aad",
            "066590890b164e57a587903bcf4b56c9",
            "22010cfb309a4b478699994d004f881a",
            "bcd2e9d6c25340f4a010d82798e7a8fa",
            "03a14efbba7d4942bee0236d62cd2eb4",
            "fe2c4058695b4922810bb33d0583cca2",
            "19ccb27d63ad482396b1720d595a8613",
            "6164c939080547b18e05e94fb2485d5f",
            "695ce0971d6046ac8be4262f84a1a805",
            "bb564f2c1867422597b74bdb044c17d1",
            "6028a7a1e604458c85cd8c74793cefc1",
            "ef52d8a4862d455798c237619dc8c303",
            "df65593685b540adbf7378e716b61b0b",
            "6e9fe604337d47bea47ab39e73366e90",
            "99c4eeff42474eb2b80cfbd5da32b237",
            "c6e32a88ea2342aaa8cdd161a0cb79d0",
            "a0bcf17db7d644728091482a3561dfb3",
            "adc9577601f3407bbf060cb8bf8fb140",
            "06277a50d9dc435c9d8a20646e753145",
            "c8498da0e12d40538b992f2875d0345c",
            "911e6bdf1c3546e3bb303fe2ef59e32a",
            "60f511b911124a69938c989421347544",
            "b500d477eac44932a66d6764b58a6c51",
            "8f35d87715c347f294ec98efdf7335be",
            "d3e137175b11421ea1e7af7d2ee22b32",
            "7d3ce24d97094d2cacd1f5c999593d8a",
            "cd9d2eca0bd44e78b58d4f00d5741147",
            "d61024740aaf42898f94a37216a5d879",
            "8615bffa919c41128a2b28b639897adf",
            "aa577a41df3445b49ea439a505ee1b49",
            "c17ab0fe197041cea5b3f507abcc71e8",
            "9c92d69d4d4e403589511d33f47fad60",
            "50fb7871b1444ee698a50fe9baee03fa",
            "d1016b3dae3b4dd6b19e23789b730a0b",
            "288e0e788dd544b88e61ed5898bd4827",
            "c8acfdf2234e4d97b433d90fe211cb8b",
            "9cc4cdab2d42424db75026d70aaa681e",
            "14888ec280844e998fc22c739d9d73c0",
            "fe7cdb20d9a040f9b3341a53b2e4ad82",
            "43cc395cc1ff4eb2b995fa58e76e52ad",
            "a7b6fce9464c4f2896c7d1b5ca319e1c",
            "60c4b86361b74ade9b2fb2e1ca55f1d6",
            "48282b23fb0c4f71874d1c97926f0c9a",
            "00fee8f5315540d4ac1095ae2d600c36",
            "2e30af5c9403446c83307f5775d08ee6",
            "5db3a966049041aab56567f27ce039cd",
            "63dc2724e36f4ee59a41951d086ac278",
            "f19ac0e9fff346eb8ca05b7149d1a743",
            "0078a9ccf62241ef874e804b4de7579a",
            "6b97a42dcc284a48b9d543a9fddd213d",
            "0ea459e2528a4d3bb7a92f43b59711e9",
            "b238117d077241d5b94e2cbad586a396",
            "268a1b0dfd2048ce9eaa6a522d8a37f1",
            "ee70eca004884306ab9648235a067536",
            "7df32b54ffc14e0ea439dfa1b3ed51d8",
            "602fe48f87294e91823b843cb37171d7",
            "f159fa742b3b4c80b4da7a3213786833",
            "479634810c9d4118bc0be598ba1778ca",
            "fc6fe7301b414ac29bdc13e672493bad",
            "776332ea2eae460f820d3d3c5a65ea72",
            "1d74da19601444e4b555318f72d42e07",
            "379e7d69497c451795660a87320fdde4",
            "a8918db26f1d400e9605ae900ae6d397",
            "77fa91709eec40dd85b492bbc8d720a8",
            "e306a34eae7f4018a06df1edd87f9497",
            "81b14ebb74bf41528495f5bbeb604c4a",
            "0849a59739b84fb49784b0d3aacd445c",
            "e25aee59832b465bafa710dba82b5f28",
            "e09eac9b32c647ee84fb07002fef89db",
            "6c9ba9da1a4542e591fb0e9bed297f9e",
            "b2e0c3e64aa04c6c8f0b2c422854368b",
            "2da63dc0957b48d29cd9d145e35f40cb",
            "045affdb81a64fd59009d69dce988547",
            "8350bf2fc4614a2f9c440cb0461f3527",
            "b39f935295e94afeaba55e777663e145",
            "b1b038a194ab43cdac782b32bd6b9224",
            "0d30af8833b54394948622f87e53a914",
            "5a34b9a00ba24adfb8fb0b18dbd9f3df",
            "e5fc00a8117c442e8d5e1b5233d3c8e4",
            "3a587abeb37c4a04aabfc618aa1cca54",
            "c4acfe1df54640b9b5701130e3ab3f78",
            "ec6eacf40f98468494ca2dd799112131",
            "a3201657ad6744a099be5e10649539f6",
            "b86d3e0286604c49a689818f820ba0b5",
            "18d893364c524a49bfb7b761217554e8",
            "0a569619fbd84da3ae9685237607ea16",
            "d091dcc49b404dd0b74ae4723c6c5936",
            "b7f52ab1ec6940a58e8de8a2a23ea08b",
            "6ff0c7f3b4a64d50af60291d60dca924",
            "aa8d8ab2e4e645eaa212e24c6faedfef",
            "928c2278624d49d4bf3bd7a3a250c832",
            "f5e508a3668f4acea917346747e466e8",
            "3ace956b175b4736a00dc4dd43978074",
            "86e29c488d9944a7ba4fc922112f057f",
            "3122c574c2d74ace9003724613f21c87",
            "4d55f2dd16f947dd98696a3294746d90",
            "4e05f4896e714b08a79f71c49bacee10",
            "615ea92961264143b4e8472a3ad5e9c5",
            "8c3c3495117142ed882f38414d43cdca",
            "d4092c967bdc492da9189234f5c88dfb",
            "7d4f498051f748eeb59c2c6575183808",
            "3faefa0e83f64162900eb4b5331dc4d5",
            "e9be4aa425c543b8adfe50bb31d88435",
            "fc5d408572fe4a7baeeb01a9d5700592",
            "7833a915bb954f1cb88c0db70bb0a7cb",
            "0a2afe494b32470280f10ee056e479b2",
            "4f0235d3aa8d416880d20444cafed3b2",
            "1e32dd6f24a140b0b882ccdc5b05f02d",
            "aee58f53f0c94f19a735dd4cb9e36975",
            "be88a7b34060498e9f6147e4194217af",
            "4d3b559a15cd4344a6c3ae27ee93f536",
            "14a85af24bb947d9b0d63e223d136fda",
            "247b2cf30df2494ba99c62a06f063e68",
            "3a2eeed3aa2643249261504ed2915f5a",
            "68e6dc4ed0b646caa2b0f4c58cb19ba8",
            "96a190e04cfd48268c52cff9e51215f6"
          ]
        },
        "id": "18577iFW6GpJ",
        "outputId": "c70fb1e3-2190-4066-847a-9aa092d0cd67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/627 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f5ba8559327a4747890bc384f5f9af58"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/13.5k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "19ccb27d63ad482396b1720d595a8613"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "adc9577601f3407bbf060cb8bf8fb140"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8615bffa919c41128a2b28b639897adf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/67.1M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "43cc395cc1ff4eb2b995fa58e76e52ad"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0ea459e2528a4d3bb7a92f43b59711e9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "379e7d69497c451795660a87320fdde4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/33.6k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "045affdb81a64fd59009d69dce988547"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b86d3e0286604c49a689818f820ba0b5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3122c574c2d74ace9003724613f21c87"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0a2afe494b32470280f10ee056e479b2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gemma 1.0 4Bit Model size: 1.8995556831359863 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"What is a language model?\"\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model_gemma1.generate(\n",
        "        **input_ids,\n",
        "        max_new_tokens=256,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "print(tokenizer.decode(outputs[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n92lqx1x6GCu",
        "outputId": "b3f56b5d-16d7-469b-b40e-070bd4b56af6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bos>What is a language model?\n",
            "Language models are a key component of many machine learning tasks, including text classification, content-based recommender systems, and natural language generation. The main idea is that we train a neural network to predict the next token in a sequence of tokens. The tokens are typically words, but can also be any other type of token, such as characters or phrases. The model learns to predict the next token based on the previous tokens and the context in the surrounding tokens.\n",
            "\n",
            "In this blog post, we will dive into the world of language models, and show you how to train your own. We will also show you how to use these models to improve the quality of your machine learning predictions.\n",
            "\n",
            "What is a language model?\n",
            "A language model is a type of machine learning model that is used to predict the next token in a sequence of tokens. The tokens are typically words, but can also be any other type of token, such as characters or phrases. Language models are used in a variety of machine learning tasks, including text classification, content-based recommender systems, and natural language generation.\n",
            "\n",
            "How does a language model work?\n",
            "A language model is a neural network that is trained on a large corpus of text. The network is trained to predict the next token in\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reference:\n",
        "1. https://colab.research.google.com/drive/1NlHlHU-fdubXcuZ08eb7zpaidF7388r6?usp=sharing\n",
        "2. https://www.youtube.com/watch?v=3EDI4akymhA"
      ],
      "metadata": {
        "id": "BVUpkg0rEG0o"
      }
    }
  ]
}